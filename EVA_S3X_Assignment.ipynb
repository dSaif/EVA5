{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of EVA S3X Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dSaif/EVA5/blob/main/EVA_S3X_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtKvmZx-WmUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37d1dadf-abd2-4c7f-ee1e-9233eabbbae9"
      },
      "source": [
        "# Installing Pytorch\n",
        "\n",
        "!pip install torch\n",
        "!pip install torchvision"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.0->torchvision) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGU6NwlsXFSt"
      },
      "source": [
        "#Import Dependencies\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bNfVLRUYqZA"
      },
      "source": [
        "#Define Hyperparameters\n",
        "\n",
        "num_classes = 62 # number of output classes discrete range [0,9]\n",
        "num_epochs = 20 # number of times which the entire dataset is passed throughout the model\n",
        "batch_size = 100 # the size of input data took for one iteration\n",
        "lr = 1e-3 # size of step "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCsBCXMwbpH5"
      },
      "source": [
        "#Downloading EMNIST data\n",
        "\n",
        "train_data = dsets.EMNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(),\n",
        "                        download = True,\n",
        "                        split='byclass')\n",
        "\n",
        "test_data = dsets.EMNIST(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor(),\n",
        "                       split='byclass')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfDPBdnYgfGp"
      },
      "source": [
        "#Loading the data\n",
        "\n",
        "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = batch_size,\n",
        "                                             shuffle = True)\n",
        "\n",
        "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = batch_size, \n",
        "                                      shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL-YXTvghaz_"
      },
      "source": [
        "#Define model class\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_classes):\n",
        "    super(Net,self).__init__()\n",
        "    self.conv1= nn.Conv2d(in_channels= 1, out_channels= 10, kernel_size= 3, padding= 1) #28x28x1 > 28x28x10; RF: 3x3\n",
        "    self.conv2= nn.Conv2d(in_channels= 10, out_channels= 10, kernel_size= 3,padding= 1 ) #28x28x10 > 28x28x10; RF: 5x5\n",
        "    self.max_pool1= nn.MaxPool2d(2, stride= 2) #28x28x10 > 14x14x10 ; RF: 10x10\n",
        "    self.conv3= nn.Conv2d(in_channels= 10, out_channels= 20, kernel_size= 3, padding=1 ) #14x14x10 > 14x14x20; RF: 12x12\n",
        "    self.conv4= nn.Conv2d(in_channels= 20, out_channels= 20, kernel_size= 3, padding= 1) #14x14x20 > 14x14x20; RF: 14x14\n",
        "    self.max_pool2= nn.MaxPool2d(2, stride= 2) #14x14x20 > 7x7x20 ; RF: 28x28\n",
        "    self.conv5= nn.Conv2d(in_channels= 20, out_channels=30, kernel_size= 3 ) #7x7x20 > 5x5x30; RF: 30x30\n",
        "    self.conv6= nn.Conv2d(in_channels= 30, out_channels= 30, kernel_size= 3) #5x5x30 > 3x3x30; RF: 32x32\n",
        "    self.avg_pool= nn.AvgPool2d(3, stride= 2) # 3x3x30 > 1x1; RF: 34x34\n",
        "    \n",
        "    # self.fc1 = nn.Linear(30*3*3, 62)\n",
        "    # self.relu = nn.ReLU()\n",
        "    # self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "  \n",
        "  def forward(self,t):\n",
        "    t= self.max_pool1(F.relu(self.conv2(F.relu(self.conv1(t)))))\n",
        "    t= self.max_pool2(F.relu(self.conv4(F.relu(self.conv3(t)))))\n",
        "    t= self.conv6(F.relu(self.conv5(t)))\n",
        "    t= t.reshape(-1, 30*3*3)\n",
        "    out = self.avg_pool(t)\n",
        "    # out = self.relu(out)\n",
        "    # out = self.fc2(out)\n",
        "    return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3EPEqbjjfAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f83c9a13-78c1-44b3-b230-34dba7256bd2"
      },
      "source": [
        "#Build the model\n",
        "# !pip install torchsummary\n",
        "from torchsummary import summary\n",
        "\n",
        "net = Net(input_size, hidden_size, num_classes)\n",
        "if torch.cuda.is_available():\n",
        "  net.cuda()\n",
        "\n",
        "summary(net, input_size=(1, 28, 28))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 28, 28]             100\n",
            "            Conv2d-2           [-1, 10, 28, 28]             910\n",
            "         MaxPool2d-3           [-1, 10, 14, 14]               0\n",
            "            Conv2d-4           [-1, 20, 14, 14]           1,820\n",
            "            Conv2d-5           [-1, 20, 14, 14]           3,620\n",
            "         MaxPool2d-6             [-1, 20, 7, 7]               0\n",
            "            Conv2d-7             [-1, 30, 5, 5]           5,430\n",
            "            Conv2d-8             [-1, 30, 3, 3]           8,130\n",
            "            Linear-9                   [-1, 62]          16,802\n",
            "================================================================\n",
            "Total params: 36,812\n",
            "Trainable params: 36,812\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.21\n",
            "Params size (MB): 0.14\n",
            "Estimated Total Size (MB): 0.35\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePLIwvAFj2zH"
      },
      "source": [
        "#Define loss-function & optimizer\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam( net.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u75Xa5VckuTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24615bb7-f44a-4fcd-e21b-31b4e1695be6"
      },
      "source": [
        "#Training the model\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i ,(images,labels) in enumerate(train_gen):\n",
        "    images = Variable(images).cuda()\n",
        "    labels = Variable(labels).cuda()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(images)\n",
        "    loss = loss_function(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (i+1) % 100 == 0:\n",
        "      print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
        "                 %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, loss.item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [100/6979], Loss: 3.8561\n",
            "Epoch [1/20], Step [200/6979], Loss: 3.5252\n",
            "Epoch [1/20], Step [300/6979], Loss: 3.0447\n",
            "Epoch [1/20], Step [400/6979], Loss: 2.2377\n",
            "Epoch [1/20], Step [500/6979], Loss: 2.0652\n",
            "Epoch [1/20], Step [600/6979], Loss: 1.9581\n",
            "Epoch [1/20], Step [700/6979], Loss: 1.4592\n",
            "Epoch [1/20], Step [800/6979], Loss: 1.6453\n",
            "Epoch [1/20], Step [900/6979], Loss: 1.5403\n",
            "Epoch [1/20], Step [1000/6979], Loss: 1.9473\n",
            "Epoch [1/20], Step [1100/6979], Loss: 1.4642\n",
            "Epoch [1/20], Step [1200/6979], Loss: 1.0996\n",
            "Epoch [1/20], Step [1300/6979], Loss: 1.2258\n",
            "Epoch [1/20], Step [1400/6979], Loss: 1.5592\n",
            "Epoch [1/20], Step [1500/6979], Loss: 0.9907\n",
            "Epoch [1/20], Step [1600/6979], Loss: 1.1812\n",
            "Epoch [1/20], Step [1700/6979], Loss: 1.3483\n",
            "Epoch [1/20], Step [1800/6979], Loss: 1.0186\n",
            "Epoch [1/20], Step [1900/6979], Loss: 1.1534\n",
            "Epoch [1/20], Step [2000/6979], Loss: 1.0380\n",
            "Epoch [1/20], Step [2100/6979], Loss: 1.1537\n",
            "Epoch [1/20], Step [2200/6979], Loss: 1.0123\n",
            "Epoch [1/20], Step [2300/6979], Loss: 1.1226\n",
            "Epoch [1/20], Step [2400/6979], Loss: 1.3393\n",
            "Epoch [1/20], Step [2500/6979], Loss: 1.3259\n",
            "Epoch [1/20], Step [2600/6979], Loss: 0.6985\n",
            "Epoch [1/20], Step [2700/6979], Loss: 1.0403\n",
            "Epoch [1/20], Step [2800/6979], Loss: 1.1417\n",
            "Epoch [1/20], Step [2900/6979], Loss: 0.8735\n",
            "Epoch [1/20], Step [3000/6979], Loss: 0.6755\n",
            "Epoch [1/20], Step [3100/6979], Loss: 0.8834\n",
            "Epoch [1/20], Step [3200/6979], Loss: 1.1244\n",
            "Epoch [1/20], Step [3300/6979], Loss: 0.9827\n",
            "Epoch [1/20], Step [3400/6979], Loss: 0.8271\n",
            "Epoch [1/20], Step [3500/6979], Loss: 0.8349\n",
            "Epoch [1/20], Step [3600/6979], Loss: 0.8591\n",
            "Epoch [1/20], Step [3700/6979], Loss: 1.0836\n",
            "Epoch [1/20], Step [3800/6979], Loss: 0.7828\n",
            "Epoch [1/20], Step [3900/6979], Loss: 1.2259\n",
            "Epoch [1/20], Step [4000/6979], Loss: 0.8829\n",
            "Epoch [1/20], Step [4100/6979], Loss: 1.0811\n",
            "Epoch [1/20], Step [4200/6979], Loss: 1.0813\n",
            "Epoch [1/20], Step [4300/6979], Loss: 0.8268\n",
            "Epoch [1/20], Step [4400/6979], Loss: 0.8767\n",
            "Epoch [1/20], Step [4500/6979], Loss: 0.7583\n",
            "Epoch [1/20], Step [4600/6979], Loss: 0.7469\n",
            "Epoch [1/20], Step [4700/6979], Loss: 0.7352\n",
            "Epoch [1/20], Step [4800/6979], Loss: 0.7792\n",
            "Epoch [1/20], Step [4900/6979], Loss: 0.7848\n",
            "Epoch [1/20], Step [5000/6979], Loss: 0.7689\n",
            "Epoch [1/20], Step [5100/6979], Loss: 0.7475\n",
            "Epoch [1/20], Step [5200/6979], Loss: 0.5883\n",
            "Epoch [1/20], Step [5300/6979], Loss: 1.0056\n",
            "Epoch [1/20], Step [5400/6979], Loss: 0.8559\n",
            "Epoch [1/20], Step [5500/6979], Loss: 0.7331\n",
            "Epoch [1/20], Step [5600/6979], Loss: 0.8730\n",
            "Epoch [1/20], Step [5700/6979], Loss: 0.8280\n",
            "Epoch [1/20], Step [5800/6979], Loss: 0.9319\n",
            "Epoch [1/20], Step [5900/6979], Loss: 0.6629\n",
            "Epoch [1/20], Step [6000/6979], Loss: 0.8172\n",
            "Epoch [1/20], Step [6100/6979], Loss: 0.8251\n",
            "Epoch [1/20], Step [6200/6979], Loss: 0.7366\n",
            "Epoch [1/20], Step [6300/6979], Loss: 0.6785\n",
            "Epoch [1/20], Step [6400/6979], Loss: 1.0242\n",
            "Epoch [1/20], Step [6500/6979], Loss: 0.7540\n",
            "Epoch [1/20], Step [6600/6979], Loss: 0.7155\n",
            "Epoch [1/20], Step [6700/6979], Loss: 0.8365\n",
            "Epoch [1/20], Step [6800/6979], Loss: 0.6052\n",
            "Epoch [1/20], Step [6900/6979], Loss: 0.8019\n",
            "Epoch [2/20], Step [100/6979], Loss: 0.5037\n",
            "Epoch [2/20], Step [200/6979], Loss: 0.7424\n",
            "Epoch [2/20], Step [300/6979], Loss: 0.9221\n",
            "Epoch [2/20], Step [400/6979], Loss: 0.8752\n",
            "Epoch [2/20], Step [500/6979], Loss: 0.6394\n",
            "Epoch [2/20], Step [600/6979], Loss: 0.8068\n",
            "Epoch [2/20], Step [700/6979], Loss: 0.6988\n",
            "Epoch [2/20], Step [800/6979], Loss: 0.9094\n",
            "Epoch [2/20], Step [900/6979], Loss: 0.6667\n",
            "Epoch [2/20], Step [1000/6979], Loss: 0.9378\n",
            "Epoch [2/20], Step [1100/6979], Loss: 0.6574\n",
            "Epoch [2/20], Step [1200/6979], Loss: 0.7681\n",
            "Epoch [2/20], Step [1300/6979], Loss: 0.9099\n",
            "Epoch [2/20], Step [1400/6979], Loss: 0.6752\n",
            "Epoch [2/20], Step [1500/6979], Loss: 0.7085\n",
            "Epoch [2/20], Step [1600/6979], Loss: 0.4852\n",
            "Epoch [2/20], Step [1700/6979], Loss: 0.4658\n",
            "Epoch [2/20], Step [1800/6979], Loss: 0.9922\n",
            "Epoch [2/20], Step [1900/6979], Loss: 0.9786\n",
            "Epoch [2/20], Step [2000/6979], Loss: 0.9476\n",
            "Epoch [2/20], Step [2100/6979], Loss: 0.5430\n",
            "Epoch [2/20], Step [2200/6979], Loss: 1.0169\n",
            "Epoch [2/20], Step [2300/6979], Loss: 0.7637\n",
            "Epoch [2/20], Step [2400/6979], Loss: 0.5965\n",
            "Epoch [2/20], Step [2500/6979], Loss: 0.7515\n",
            "Epoch [2/20], Step [2600/6979], Loss: 0.9570\n",
            "Epoch [2/20], Step [2700/6979], Loss: 0.7085\n",
            "Epoch [2/20], Step [2800/6979], Loss: 0.8614\n",
            "Epoch [2/20], Step [2900/6979], Loss: 0.8493\n",
            "Epoch [2/20], Step [3000/6979], Loss: 0.8508\n",
            "Epoch [2/20], Step [3100/6979], Loss: 0.8061\n",
            "Epoch [2/20], Step [3200/6979], Loss: 0.8769\n",
            "Epoch [2/20], Step [3300/6979], Loss: 0.8340\n",
            "Epoch [2/20], Step [3400/6979], Loss: 0.5824\n",
            "Epoch [2/20], Step [3500/6979], Loss: 0.7644\n",
            "Epoch [2/20], Step [3600/6979], Loss: 0.8199\n",
            "Epoch [2/20], Step [3700/6979], Loss: 0.7270\n",
            "Epoch [2/20], Step [3800/6979], Loss: 0.8086\n",
            "Epoch [2/20], Step [3900/6979], Loss: 0.7689\n",
            "Epoch [2/20], Step [4000/6979], Loss: 0.7418\n",
            "Epoch [2/20], Step [4100/6979], Loss: 0.8770\n",
            "Epoch [2/20], Step [4200/6979], Loss: 0.7565\n",
            "Epoch [2/20], Step [4300/6979], Loss: 0.7090\n",
            "Epoch [2/20], Step [4400/6979], Loss: 0.6186\n",
            "Epoch [2/20], Step [4500/6979], Loss: 0.8414\n",
            "Epoch [2/20], Step [4600/6979], Loss: 0.6838\n",
            "Epoch [2/20], Step [4700/6979], Loss: 0.8064\n",
            "Epoch [2/20], Step [4800/6979], Loss: 0.6751\n",
            "Epoch [2/20], Step [4900/6979], Loss: 0.4731\n",
            "Epoch [2/20], Step [5000/6979], Loss: 0.7398\n",
            "Epoch [2/20], Step [5100/6979], Loss: 0.8447\n",
            "Epoch [2/20], Step [5200/6979], Loss: 0.7652\n",
            "Epoch [2/20], Step [5300/6979], Loss: 0.9322\n",
            "Epoch [2/20], Step [5400/6979], Loss: 0.5576\n",
            "Epoch [2/20], Step [5500/6979], Loss: 0.7547\n",
            "Epoch [2/20], Step [5600/6979], Loss: 0.5768\n",
            "Epoch [2/20], Step [5700/6979], Loss: 0.5724\n",
            "Epoch [2/20], Step [5800/6979], Loss: 0.7075\n",
            "Epoch [2/20], Step [5900/6979], Loss: 0.9090\n",
            "Epoch [2/20], Step [6000/6979], Loss: 0.6408\n",
            "Epoch [2/20], Step [6100/6979], Loss: 0.5075\n",
            "Epoch [2/20], Step [6200/6979], Loss: 0.5313\n",
            "Epoch [2/20], Step [6300/6979], Loss: 0.6812\n",
            "Epoch [2/20], Step [6400/6979], Loss: 0.5863\n",
            "Epoch [2/20], Step [6500/6979], Loss: 0.5135\n",
            "Epoch [2/20], Step [6600/6979], Loss: 0.5327\n",
            "Epoch [2/20], Step [6700/6979], Loss: 0.7692\n",
            "Epoch [2/20], Step [6800/6979], Loss: 0.8924\n",
            "Epoch [2/20], Step [6900/6979], Loss: 0.9383\n",
            "Epoch [3/20], Step [100/6979], Loss: 0.6202\n",
            "Epoch [3/20], Step [200/6979], Loss: 0.7175\n",
            "Epoch [3/20], Step [300/6979], Loss: 0.5407\n",
            "Epoch [3/20], Step [400/6979], Loss: 0.6129\n",
            "Epoch [3/20], Step [500/6979], Loss: 0.5239\n",
            "Epoch [3/20], Step [600/6979], Loss: 0.7991\n",
            "Epoch [3/20], Step [700/6979], Loss: 0.5728\n",
            "Epoch [3/20], Step [800/6979], Loss: 0.4385\n",
            "Epoch [3/20], Step [900/6979], Loss: 0.6410\n",
            "Epoch [3/20], Step [1000/6979], Loss: 0.4418\n",
            "Epoch [3/20], Step [1100/6979], Loss: 0.6894\n",
            "Epoch [3/20], Step [1200/6979], Loss: 0.7526\n",
            "Epoch [3/20], Step [1300/6979], Loss: 0.5309\n",
            "Epoch [3/20], Step [1400/6979], Loss: 0.5082\n",
            "Epoch [3/20], Step [1500/6979], Loss: 0.6246\n",
            "Epoch [3/20], Step [1600/6979], Loss: 0.4812\n",
            "Epoch [3/20], Step [1700/6979], Loss: 0.7043\n",
            "Epoch [3/20], Step [1800/6979], Loss: 0.4797\n",
            "Epoch [3/20], Step [1900/6979], Loss: 0.5601\n",
            "Epoch [3/20], Step [2000/6979], Loss: 0.5831\n",
            "Epoch [3/20], Step [2100/6979], Loss: 0.6638\n",
            "Epoch [3/20], Step [2200/6979], Loss: 0.6239\n",
            "Epoch [3/20], Step [2300/6979], Loss: 0.6202\n",
            "Epoch [3/20], Step [2400/6979], Loss: 0.7625\n",
            "Epoch [3/20], Step [2500/6979], Loss: 0.5271\n",
            "Epoch [3/20], Step [2600/6979], Loss: 0.7205\n",
            "Epoch [3/20], Step [2700/6979], Loss: 0.6357\n",
            "Epoch [3/20], Step [2800/6979], Loss: 0.8417\n",
            "Epoch [3/20], Step [2900/6979], Loss: 0.6033\n",
            "Epoch [3/20], Step [3000/6979], Loss: 0.5960\n",
            "Epoch [3/20], Step [3100/6979], Loss: 0.6326\n",
            "Epoch [3/20], Step [3200/6979], Loss: 0.3816\n",
            "Epoch [3/20], Step [3300/6979], Loss: 0.7479\n",
            "Epoch [3/20], Step [3400/6979], Loss: 0.7356\n",
            "Epoch [3/20], Step [3500/6979], Loss: 0.8156\n",
            "Epoch [3/20], Step [3600/6979], Loss: 0.5366\n",
            "Epoch [3/20], Step [3700/6979], Loss: 0.7163\n",
            "Epoch [3/20], Step [3800/6979], Loss: 0.8309\n",
            "Epoch [3/20], Step [3900/6979], Loss: 0.5022\n",
            "Epoch [3/20], Step [4000/6979], Loss: 0.7341\n",
            "Epoch [3/20], Step [4100/6979], Loss: 0.5362\n",
            "Epoch [3/20], Step [4200/6979], Loss: 0.6016\n",
            "Epoch [3/20], Step [4300/6979], Loss: 0.4948\n",
            "Epoch [3/20], Step [4400/6979], Loss: 0.5835\n",
            "Epoch [3/20], Step [4500/6979], Loss: 0.4251\n",
            "Epoch [3/20], Step [4600/6979], Loss: 0.6105\n",
            "Epoch [3/20], Step [4700/6979], Loss: 0.6989\n",
            "Epoch [3/20], Step [4800/6979], Loss: 0.6666\n",
            "Epoch [3/20], Step [4900/6979], Loss: 0.5907\n",
            "Epoch [3/20], Step [5000/6979], Loss: 0.4829\n",
            "Epoch [3/20], Step [5100/6979], Loss: 0.8494\n",
            "Epoch [3/20], Step [5200/6979], Loss: 0.5960\n",
            "Epoch [3/20], Step [5300/6979], Loss: 0.5127\n",
            "Epoch [3/20], Step [5400/6979], Loss: 0.5271\n",
            "Epoch [3/20], Step [5500/6979], Loss: 0.5678\n",
            "Epoch [3/20], Step [5600/6979], Loss: 0.7372\n",
            "Epoch [3/20], Step [5700/6979], Loss: 0.6102\n",
            "Epoch [3/20], Step [5800/6979], Loss: 0.7061\n",
            "Epoch [3/20], Step [5900/6979], Loss: 0.4783\n",
            "Epoch [3/20], Step [6000/6979], Loss: 0.4911\n",
            "Epoch [3/20], Step [6100/6979], Loss: 0.6486\n",
            "Epoch [3/20], Step [6200/6979], Loss: 0.6021\n",
            "Epoch [3/20], Step [6300/6979], Loss: 0.5734\n",
            "Epoch [3/20], Step [6400/6979], Loss: 0.4676\n",
            "Epoch [3/20], Step [6500/6979], Loss: 0.6674\n",
            "Epoch [3/20], Step [6600/6979], Loss: 0.4927\n",
            "Epoch [3/20], Step [6700/6979], Loss: 0.5647\n",
            "Epoch [3/20], Step [6800/6979], Loss: 0.5905\n",
            "Epoch [3/20], Step [6900/6979], Loss: 0.7490\n",
            "Epoch [4/20], Step [100/6979], Loss: 0.5995\n",
            "Epoch [4/20], Step [200/6979], Loss: 0.8057\n",
            "Epoch [4/20], Step [300/6979], Loss: 0.5439\n",
            "Epoch [4/20], Step [400/6979], Loss: 0.6218\n",
            "Epoch [4/20], Step [500/6979], Loss: 0.6208\n",
            "Epoch [4/20], Step [600/6979], Loss: 0.4329\n",
            "Epoch [4/20], Step [700/6979], Loss: 0.5286\n",
            "Epoch [4/20], Step [800/6979], Loss: 0.5330\n",
            "Epoch [4/20], Step [900/6979], Loss: 0.6915\n",
            "Epoch [4/20], Step [1000/6979], Loss: 0.7076\n",
            "Epoch [4/20], Step [1100/6979], Loss: 0.5929\n",
            "Epoch [4/20], Step [1200/6979], Loss: 0.4892\n",
            "Epoch [4/20], Step [1300/6979], Loss: 0.5767\n",
            "Epoch [4/20], Step [1400/6979], Loss: 0.4469\n",
            "Epoch [4/20], Step [1500/6979], Loss: 0.4440\n",
            "Epoch [4/20], Step [1600/6979], Loss: 0.5113\n",
            "Epoch [4/20], Step [1700/6979], Loss: 0.7273\n",
            "Epoch [4/20], Step [1800/6979], Loss: 0.3096\n",
            "Epoch [4/20], Step [1900/6979], Loss: 0.4622\n",
            "Epoch [4/20], Step [2000/6979], Loss: 0.8750\n",
            "Epoch [4/20], Step [2100/6979], Loss: 0.4966\n",
            "Epoch [4/20], Step [2200/6979], Loss: 0.6221\n",
            "Epoch [4/20], Step [2300/6979], Loss: 0.4916\n",
            "Epoch [4/20], Step [2400/6979], Loss: 0.6020\n",
            "Epoch [4/20], Step [2500/6979], Loss: 0.7302\n",
            "Epoch [4/20], Step [2600/6979], Loss: 0.6220\n",
            "Epoch [4/20], Step [2700/6979], Loss: 0.7676\n",
            "Epoch [4/20], Step [2800/6979], Loss: 0.6191\n",
            "Epoch [4/20], Step [2900/6979], Loss: 0.7183\n",
            "Epoch [4/20], Step [3000/6979], Loss: 0.4800\n",
            "Epoch [4/20], Step [3100/6979], Loss: 0.5486\n",
            "Epoch [4/20], Step [3200/6979], Loss: 0.5912\n",
            "Epoch [4/20], Step [3300/6979], Loss: 0.6985\n",
            "Epoch [4/20], Step [3400/6979], Loss: 0.5447\n",
            "Epoch [4/20], Step [3500/6979], Loss: 0.6311\n",
            "Epoch [4/20], Step [3600/6979], Loss: 0.5924\n",
            "Epoch [4/20], Step [3700/6979], Loss: 0.6095\n",
            "Epoch [4/20], Step [3800/6979], Loss: 0.6054\n",
            "Epoch [4/20], Step [3900/6979], Loss: 0.4826\n",
            "Epoch [4/20], Step [4000/6979], Loss: 0.7107\n",
            "Epoch [4/20], Step [4100/6979], Loss: 0.7917\n",
            "Epoch [4/20], Step [4200/6979], Loss: 0.8150\n",
            "Epoch [4/20], Step [4300/6979], Loss: 0.5410\n",
            "Epoch [4/20], Step [4400/6979], Loss: 0.6202\n",
            "Epoch [4/20], Step [4500/6979], Loss: 0.7185\n",
            "Epoch [4/20], Step [4600/6979], Loss: 0.5431\n",
            "Epoch [4/20], Step [4700/6979], Loss: 0.5867\n",
            "Epoch [4/20], Step [4800/6979], Loss: 0.5125\n",
            "Epoch [4/20], Step [4900/6979], Loss: 0.5172\n",
            "Epoch [4/20], Step [5000/6979], Loss: 0.5528\n",
            "Epoch [4/20], Step [5100/6979], Loss: 0.4859\n",
            "Epoch [4/20], Step [5200/6979], Loss: 0.4014\n",
            "Epoch [4/20], Step [5300/6979], Loss: 0.5885\n",
            "Epoch [4/20], Step [5400/6979], Loss: 0.6081\n",
            "Epoch [4/20], Step [5500/6979], Loss: 0.7201\n",
            "Epoch [4/20], Step [5600/6979], Loss: 0.4281\n",
            "Epoch [4/20], Step [5700/6979], Loss: 0.5310\n",
            "Epoch [4/20], Step [5800/6979], Loss: 0.6086\n",
            "Epoch [4/20], Step [5900/6979], Loss: 0.4571\n",
            "Epoch [4/20], Step [6000/6979], Loss: 0.8165\n",
            "Epoch [4/20], Step [6100/6979], Loss: 0.5578\n",
            "Epoch [4/20], Step [6200/6979], Loss: 0.4264\n",
            "Epoch [4/20], Step [6300/6979], Loss: 0.6530\n",
            "Epoch [4/20], Step [6400/6979], Loss: 0.7041\n",
            "Epoch [4/20], Step [6500/6979], Loss: 0.6072\n",
            "Epoch [4/20], Step [6600/6979], Loss: 0.4707\n",
            "Epoch [4/20], Step [6700/6979], Loss: 0.5726\n",
            "Epoch [4/20], Step [6800/6979], Loss: 0.6801\n",
            "Epoch [4/20], Step [6900/6979], Loss: 0.5446\n",
            "Epoch [5/20], Step [100/6979], Loss: 0.7314\n",
            "Epoch [5/20], Step [200/6979], Loss: 0.8494\n",
            "Epoch [5/20], Step [300/6979], Loss: 0.5848\n",
            "Epoch [5/20], Step [400/6979], Loss: 0.4732\n",
            "Epoch [5/20], Step [500/6979], Loss: 0.6047\n",
            "Epoch [5/20], Step [600/6979], Loss: 0.5513\n",
            "Epoch [5/20], Step [700/6979], Loss: 0.5967\n",
            "Epoch [5/20], Step [800/6979], Loss: 0.4783\n",
            "Epoch [5/20], Step [900/6979], Loss: 0.4686\n",
            "Epoch [5/20], Step [1000/6979], Loss: 0.6539\n",
            "Epoch [5/20], Step [1100/6979], Loss: 0.4220\n",
            "Epoch [5/20], Step [1200/6979], Loss: 0.6492\n",
            "Epoch [5/20], Step [1300/6979], Loss: 0.7490\n",
            "Epoch [5/20], Step [1400/6979], Loss: 0.7689\n",
            "Epoch [5/20], Step [1500/6979], Loss: 0.7330\n",
            "Epoch [5/20], Step [1600/6979], Loss: 0.6553\n",
            "Epoch [5/20], Step [1700/6979], Loss: 0.6162\n",
            "Epoch [5/20], Step [1800/6979], Loss: 0.6575\n",
            "Epoch [5/20], Step [1900/6979], Loss: 0.5047\n",
            "Epoch [5/20], Step [2000/6979], Loss: 0.6493\n",
            "Epoch [5/20], Step [2100/6979], Loss: 0.6912\n",
            "Epoch [5/20], Step [2200/6979], Loss: 0.4569\n",
            "Epoch [5/20], Step [2300/6979], Loss: 0.4188\n",
            "Epoch [5/20], Step [2400/6979], Loss: 0.4932\n",
            "Epoch [5/20], Step [2500/6979], Loss: 0.5154\n",
            "Epoch [5/20], Step [2600/6979], Loss: 0.6320\n",
            "Epoch [5/20], Step [2700/6979], Loss: 0.5410\n",
            "Epoch [5/20], Step [2800/6979], Loss: 0.4357\n",
            "Epoch [5/20], Step [2900/6979], Loss: 0.5691\n",
            "Epoch [5/20], Step [3000/6979], Loss: 0.3747\n",
            "Epoch [5/20], Step [3100/6979], Loss: 0.4484\n",
            "Epoch [5/20], Step [3200/6979], Loss: 0.6139\n",
            "Epoch [5/20], Step [3300/6979], Loss: 0.5253\n",
            "Epoch [5/20], Step [3400/6979], Loss: 0.5917\n",
            "Epoch [5/20], Step [3500/6979], Loss: 0.4962\n",
            "Epoch [5/20], Step [3600/6979], Loss: 0.3143\n",
            "Epoch [5/20], Step [3700/6979], Loss: 0.3992\n",
            "Epoch [5/20], Step [3800/6979], Loss: 0.6424\n",
            "Epoch [5/20], Step [3900/6979], Loss: 0.5532\n",
            "Epoch [5/20], Step [4000/6979], Loss: 0.5108\n",
            "Epoch [5/20], Step [4100/6979], Loss: 0.5108\n",
            "Epoch [5/20], Step [4200/6979], Loss: 0.5484\n",
            "Epoch [5/20], Step [4300/6979], Loss: 0.3958\n",
            "Epoch [5/20], Step [4400/6979], Loss: 0.6749\n",
            "Epoch [5/20], Step [4500/6979], Loss: 0.5780\n",
            "Epoch [5/20], Step [4600/6979], Loss: 0.4961\n",
            "Epoch [5/20], Step [4700/6979], Loss: 0.6220\n",
            "Epoch [5/20], Step [4800/6979], Loss: 0.3232\n",
            "Epoch [5/20], Step [4900/6979], Loss: 0.4853\n",
            "Epoch [5/20], Step [5000/6979], Loss: 0.4114\n",
            "Epoch [5/20], Step [5100/6979], Loss: 0.4439\n",
            "Epoch [5/20], Step [5200/6979], Loss: 0.4936\n",
            "Epoch [5/20], Step [5300/6979], Loss: 0.5447\n",
            "Epoch [5/20], Step [5400/6979], Loss: 0.3857\n",
            "Epoch [5/20], Step [5500/6979], Loss: 0.4934\n",
            "Epoch [5/20], Step [5600/6979], Loss: 0.4788\n",
            "Epoch [5/20], Step [5700/6979], Loss: 0.5455\n",
            "Epoch [5/20], Step [5800/6979], Loss: 0.4093\n",
            "Epoch [5/20], Step [5900/6979], Loss: 0.6596\n",
            "Epoch [5/20], Step [6000/6979], Loss: 0.5524\n",
            "Epoch [5/20], Step [6100/6979], Loss: 0.5364\n",
            "Epoch [5/20], Step [6200/6979], Loss: 0.6047\n",
            "Epoch [5/20], Step [6300/6979], Loss: 0.4253\n",
            "Epoch [5/20], Step [6400/6979], Loss: 0.3994\n",
            "Epoch [5/20], Step [6500/6979], Loss: 0.7315\n",
            "Epoch [5/20], Step [6600/6979], Loss: 0.5309\n",
            "Epoch [5/20], Step [6700/6979], Loss: 0.5569\n",
            "Epoch [5/20], Step [6800/6979], Loss: 0.8332\n",
            "Epoch [5/20], Step [6900/6979], Loss: 0.4316\n",
            "Epoch [6/20], Step [100/6979], Loss: 0.7391\n",
            "Epoch [6/20], Step [200/6979], Loss: 0.6201\n",
            "Epoch [6/20], Step [300/6979], Loss: 0.6445\n",
            "Epoch [6/20], Step [400/6979], Loss: 0.7503\n",
            "Epoch [6/20], Step [500/6979], Loss: 0.5865\n",
            "Epoch [6/20], Step [600/6979], Loss: 0.6870\n",
            "Epoch [6/20], Step [700/6979], Loss: 0.6338\n",
            "Epoch [6/20], Step [800/6979], Loss: 0.6302\n",
            "Epoch [6/20], Step [900/6979], Loss: 0.4599\n",
            "Epoch [6/20], Step [1000/6979], Loss: 0.4529\n",
            "Epoch [6/20], Step [1100/6979], Loss: 0.3637\n",
            "Epoch [6/20], Step [1200/6979], Loss: 0.4719\n",
            "Epoch [6/20], Step [1300/6979], Loss: 0.5671\n",
            "Epoch [6/20], Step [1400/6979], Loss: 0.5126\n",
            "Epoch [6/20], Step [1500/6979], Loss: 0.5176\n",
            "Epoch [6/20], Step [1600/6979], Loss: 0.5377\n",
            "Epoch [6/20], Step [1700/6979], Loss: 0.5415\n",
            "Epoch [6/20], Step [1800/6979], Loss: 0.3724\n",
            "Epoch [6/20], Step [1900/6979], Loss: 0.3384\n",
            "Epoch [6/20], Step [2000/6979], Loss: 0.4493\n",
            "Epoch [6/20], Step [2100/6979], Loss: 0.6069\n",
            "Epoch [6/20], Step [2200/6979], Loss: 0.4728\n",
            "Epoch [6/20], Step [2300/6979], Loss: 0.3949\n",
            "Epoch [6/20], Step [2400/6979], Loss: 0.6282\n",
            "Epoch [6/20], Step [2500/6979], Loss: 0.5923\n",
            "Epoch [6/20], Step [2600/6979], Loss: 0.7513\n",
            "Epoch [6/20], Step [2700/6979], Loss: 0.4731\n",
            "Epoch [6/20], Step [2800/6979], Loss: 0.7995\n",
            "Epoch [6/20], Step [2900/6979], Loss: 0.4959\n",
            "Epoch [6/20], Step [3000/6979], Loss: 0.4301\n",
            "Epoch [6/20], Step [3100/6979], Loss: 0.5256\n",
            "Epoch [6/20], Step [3200/6979], Loss: 0.6587\n",
            "Epoch [6/20], Step [3300/6979], Loss: 0.4898\n",
            "Epoch [6/20], Step [3400/6979], Loss: 0.5242\n",
            "Epoch [6/20], Step [3500/6979], Loss: 0.6169\n",
            "Epoch [6/20], Step [3600/6979], Loss: 0.4979\n",
            "Epoch [6/20], Step [3700/6979], Loss: 0.3939\n",
            "Epoch [6/20], Step [3800/6979], Loss: 0.6690\n",
            "Epoch [6/20], Step [3900/6979], Loss: 0.4099\n",
            "Epoch [6/20], Step [4000/6979], Loss: 0.4557\n",
            "Epoch [6/20], Step [4100/6979], Loss: 0.4512\n",
            "Epoch [6/20], Step [4200/6979], Loss: 0.5122\n",
            "Epoch [6/20], Step [4300/6979], Loss: 0.5086\n",
            "Epoch [6/20], Step [4400/6979], Loss: 0.4453\n",
            "Epoch [6/20], Step [4500/6979], Loss: 0.6941\n",
            "Epoch [6/20], Step [4600/6979], Loss: 0.4388\n",
            "Epoch [6/20], Step [4700/6979], Loss: 0.5044\n",
            "Epoch [6/20], Step [4800/6979], Loss: 0.6895\n",
            "Epoch [6/20], Step [4900/6979], Loss: 0.6297\n",
            "Epoch [6/20], Step [5000/6979], Loss: 0.5316\n",
            "Epoch [6/20], Step [5100/6979], Loss: 0.6962\n",
            "Epoch [6/20], Step [5200/6979], Loss: 0.7915\n",
            "Epoch [6/20], Step [5300/6979], Loss: 0.4083\n",
            "Epoch [6/20], Step [5400/6979], Loss: 0.5719\n",
            "Epoch [6/20], Step [5500/6979], Loss: 0.5823\n",
            "Epoch [6/20], Step [5600/6979], Loss: 0.6106\n",
            "Epoch [6/20], Step [5700/6979], Loss: 0.6355\n",
            "Epoch [6/20], Step [5800/6979], Loss: 0.6586\n",
            "Epoch [6/20], Step [5900/6979], Loss: 0.3495\n",
            "Epoch [6/20], Step [6000/6979], Loss: 0.6583\n",
            "Epoch [6/20], Step [6100/6979], Loss: 0.6856\n",
            "Epoch [6/20], Step [6200/6979], Loss: 0.4408\n",
            "Epoch [6/20], Step [6300/6979], Loss: 0.5738\n",
            "Epoch [6/20], Step [6400/6979], Loss: 0.4655\n",
            "Epoch [6/20], Step [6500/6979], Loss: 0.4215\n",
            "Epoch [6/20], Step [6600/6979], Loss: 0.5340\n",
            "Epoch [6/20], Step [6700/6979], Loss: 0.5813\n",
            "Epoch [6/20], Step [6800/6979], Loss: 0.3126\n",
            "Epoch [6/20], Step [6900/6979], Loss: 0.6277\n",
            "Epoch [7/20], Step [100/6979], Loss: 0.6505\n",
            "Epoch [7/20], Step [200/6979], Loss: 0.6981\n",
            "Epoch [7/20], Step [300/6979], Loss: 0.4270\n",
            "Epoch [7/20], Step [400/6979], Loss: 0.5760\n",
            "Epoch [7/20], Step [500/6979], Loss: 0.5157\n",
            "Epoch [7/20], Step [600/6979], Loss: 0.5817\n",
            "Epoch [7/20], Step [700/6979], Loss: 0.5567\n",
            "Epoch [7/20], Step [800/6979], Loss: 0.3484\n",
            "Epoch [7/20], Step [900/6979], Loss: 0.4706\n",
            "Epoch [7/20], Step [1000/6979], Loss: 0.6412\n",
            "Epoch [7/20], Step [1100/6979], Loss: 0.3750\n",
            "Epoch [7/20], Step [1200/6979], Loss: 0.5000\n",
            "Epoch [7/20], Step [1300/6979], Loss: 0.5725\n",
            "Epoch [7/20], Step [1400/6979], Loss: 0.5425\n",
            "Epoch [7/20], Step [1500/6979], Loss: 0.4242\n",
            "Epoch [7/20], Step [1600/6979], Loss: 0.4510\n",
            "Epoch [7/20], Step [1700/6979], Loss: 0.4131\n",
            "Epoch [7/20], Step [1800/6979], Loss: 0.4861\n",
            "Epoch [7/20], Step [1900/6979], Loss: 0.4781\n",
            "Epoch [7/20], Step [2000/6979], Loss: 0.6862\n",
            "Epoch [7/20], Step [2100/6979], Loss: 0.5079\n",
            "Epoch [7/20], Step [2200/6979], Loss: 0.3355\n",
            "Epoch [7/20], Step [2300/6979], Loss: 0.6444\n",
            "Epoch [7/20], Step [2400/6979], Loss: 0.5650\n",
            "Epoch [7/20], Step [2500/6979], Loss: 0.5396\n",
            "Epoch [7/20], Step [2600/6979], Loss: 0.4500\n",
            "Epoch [7/20], Step [2700/6979], Loss: 0.5262\n",
            "Epoch [7/20], Step [2800/6979], Loss: 0.3413\n",
            "Epoch [7/20], Step [2900/6979], Loss: 0.4228\n",
            "Epoch [7/20], Step [3000/6979], Loss: 0.6693\n",
            "Epoch [7/20], Step [3100/6979], Loss: 0.8098\n",
            "Epoch [7/20], Step [3200/6979], Loss: 0.6176\n",
            "Epoch [7/20], Step [3300/6979], Loss: 0.5051\n",
            "Epoch [7/20], Step [3400/6979], Loss: 0.4504\n",
            "Epoch [7/20], Step [3500/6979], Loss: 0.5709\n",
            "Epoch [7/20], Step [3600/6979], Loss: 0.3946\n",
            "Epoch [7/20], Step [3700/6979], Loss: 0.6961\n",
            "Epoch [7/20], Step [3800/6979], Loss: 0.5574\n",
            "Epoch [7/20], Step [3900/6979], Loss: 0.3452\n",
            "Epoch [7/20], Step [4000/6979], Loss: 0.6247\n",
            "Epoch [7/20], Step [4100/6979], Loss: 0.5741\n",
            "Epoch [7/20], Step [4200/6979], Loss: 0.5849\n",
            "Epoch [7/20], Step [4300/6979], Loss: 0.5547\n",
            "Epoch [7/20], Step [4400/6979], Loss: 0.4674\n",
            "Epoch [7/20], Step [4500/6979], Loss: 0.5475\n",
            "Epoch [7/20], Step [4600/6979], Loss: 0.6606\n",
            "Epoch [7/20], Step [4700/6979], Loss: 0.5715\n",
            "Epoch [7/20], Step [4800/6979], Loss: 0.3858\n",
            "Epoch [7/20], Step [4900/6979], Loss: 0.4490\n",
            "Epoch [7/20], Step [5000/6979], Loss: 0.4193\n",
            "Epoch [7/20], Step [5100/6979], Loss: 0.5952\n",
            "Epoch [7/20], Step [5200/6979], Loss: 0.5177\n",
            "Epoch [7/20], Step [5300/6979], Loss: 0.3136\n",
            "Epoch [7/20], Step [5400/6979], Loss: 0.4699\n",
            "Epoch [7/20], Step [5500/6979], Loss: 0.4678\n",
            "Epoch [7/20], Step [5600/6979], Loss: 0.4173\n",
            "Epoch [7/20], Step [5700/6979], Loss: 0.5129\n",
            "Epoch [7/20], Step [5800/6979], Loss: 0.4468\n",
            "Epoch [7/20], Step [5900/6979], Loss: 0.4236\n",
            "Epoch [7/20], Step [6000/6979], Loss: 0.3636\n",
            "Epoch [7/20], Step [6100/6979], Loss: 0.4951\n",
            "Epoch [7/20], Step [6200/6979], Loss: 0.6865\n",
            "Epoch [7/20], Step [6300/6979], Loss: 0.5142\n",
            "Epoch [7/20], Step [6400/6979], Loss: 0.3829\n",
            "Epoch [7/20], Step [6500/6979], Loss: 0.3825\n",
            "Epoch [7/20], Step [6600/6979], Loss: 0.5950\n",
            "Epoch [7/20], Step [6700/6979], Loss: 0.3987\n",
            "Epoch [7/20], Step [6800/6979], Loss: 0.5797\n",
            "Epoch [7/20], Step [6900/6979], Loss: 0.6034\n",
            "Epoch [8/20], Step [100/6979], Loss: 0.3603\n",
            "Epoch [8/20], Step [200/6979], Loss: 0.3554\n",
            "Epoch [8/20], Step [300/6979], Loss: 0.6807\n",
            "Epoch [8/20], Step [400/6979], Loss: 0.6703\n",
            "Epoch [8/20], Step [500/6979], Loss: 0.4182\n",
            "Epoch [8/20], Step [600/6979], Loss: 0.4400\n",
            "Epoch [8/20], Step [700/6979], Loss: 0.3561\n",
            "Epoch [8/20], Step [800/6979], Loss: 0.5780\n",
            "Epoch [8/20], Step [900/6979], Loss: 0.3967\n",
            "Epoch [8/20], Step [1000/6979], Loss: 0.6222\n",
            "Epoch [8/20], Step [1100/6979], Loss: 0.4354\n",
            "Epoch [8/20], Step [1200/6979], Loss: 0.6045\n",
            "Epoch [8/20], Step [1300/6979], Loss: 0.5427\n",
            "Epoch [8/20], Step [1400/6979], Loss: 0.4271\n",
            "Epoch [8/20], Step [1500/6979], Loss: 0.5823\n",
            "Epoch [8/20], Step [1600/6979], Loss: 0.6280\n",
            "Epoch [8/20], Step [1700/6979], Loss: 0.4564\n",
            "Epoch [8/20], Step [1800/6979], Loss: 0.4377\n",
            "Epoch [8/20], Step [1900/6979], Loss: 0.4480\n",
            "Epoch [8/20], Step [2000/6979], Loss: 0.6766\n",
            "Epoch [8/20], Step [2100/6979], Loss: 0.4817\n",
            "Epoch [8/20], Step [2200/6979], Loss: 0.5535\n",
            "Epoch [8/20], Step [2300/6979], Loss: 0.5046\n",
            "Epoch [8/20], Step [2400/6979], Loss: 0.6033\n",
            "Epoch [8/20], Step [2500/6979], Loss: 0.5624\n",
            "Epoch [8/20], Step [2600/6979], Loss: 0.4435\n",
            "Epoch [8/20], Step [2700/6979], Loss: 0.4319\n",
            "Epoch [8/20], Step [2800/6979], Loss: 0.4610\n",
            "Epoch [8/20], Step [2900/6979], Loss: 0.6395\n",
            "Epoch [8/20], Step [3000/6979], Loss: 0.6240\n",
            "Epoch [8/20], Step [3100/6979], Loss: 0.4734\n",
            "Epoch [8/20], Step [3200/6979], Loss: 0.4202\n",
            "Epoch [8/20], Step [3300/6979], Loss: 0.5349\n",
            "Epoch [8/20], Step [3400/6979], Loss: 0.4619\n",
            "Epoch [8/20], Step [3500/6979], Loss: 0.6279\n",
            "Epoch [8/20], Step [3600/6979], Loss: 0.4621\n",
            "Epoch [8/20], Step [3700/6979], Loss: 0.7241\n",
            "Epoch [8/20], Step [3800/6979], Loss: 0.3041\n",
            "Epoch [8/20], Step [3900/6979], Loss: 0.5088\n",
            "Epoch [8/20], Step [4000/6979], Loss: 0.4478\n",
            "Epoch [8/20], Step [4100/6979], Loss: 0.5575\n",
            "Epoch [8/20], Step [4200/6979], Loss: 0.5949\n",
            "Epoch [8/20], Step [4300/6979], Loss: 0.5992\n",
            "Epoch [8/20], Step [4400/6979], Loss: 0.4843\n",
            "Epoch [8/20], Step [4500/6979], Loss: 0.3812\n",
            "Epoch [8/20], Step [4600/6979], Loss: 0.4751\n",
            "Epoch [8/20], Step [4700/6979], Loss: 0.5154\n",
            "Epoch [8/20], Step [4800/6979], Loss: 0.5815\n",
            "Epoch [8/20], Step [4900/6979], Loss: 0.5387\n",
            "Epoch [8/20], Step [5000/6979], Loss: 0.6169\n",
            "Epoch [8/20], Step [5100/6979], Loss: 0.4865\n",
            "Epoch [8/20], Step [5200/6979], Loss: 0.3416\n",
            "Epoch [8/20], Step [5300/6979], Loss: 0.3122\n",
            "Epoch [8/20], Step [5400/6979], Loss: 0.5867\n",
            "Epoch [8/20], Step [5500/6979], Loss: 0.4964\n",
            "Epoch [8/20], Step [5600/6979], Loss: 0.4768\n",
            "Epoch [8/20], Step [5700/6979], Loss: 0.4644\n",
            "Epoch [8/20], Step [5800/6979], Loss: 0.5967\n",
            "Epoch [8/20], Step [5900/6979], Loss: 0.4486\n",
            "Epoch [8/20], Step [6000/6979], Loss: 0.4519\n",
            "Epoch [8/20], Step [6100/6979], Loss: 0.4590\n",
            "Epoch [8/20], Step [6200/6979], Loss: 0.5195\n",
            "Epoch [8/20], Step [6300/6979], Loss: 0.4786\n",
            "Epoch [8/20], Step [6400/6979], Loss: 0.4798\n",
            "Epoch [8/20], Step [6500/6979], Loss: 0.5209\n",
            "Epoch [8/20], Step [6600/6979], Loss: 0.5154\n",
            "Epoch [8/20], Step [6700/6979], Loss: 0.4972\n",
            "Epoch [8/20], Step [6800/6979], Loss: 0.4205\n",
            "Epoch [8/20], Step [6900/6979], Loss: 0.6261\n",
            "Epoch [9/20], Step [100/6979], Loss: 0.6563\n",
            "Epoch [9/20], Step [200/6979], Loss: 0.3775\n",
            "Epoch [9/20], Step [300/6979], Loss: 0.3738\n",
            "Epoch [9/20], Step [400/6979], Loss: 0.2734\n",
            "Epoch [9/20], Step [500/6979], Loss: 0.5242\n",
            "Epoch [9/20], Step [600/6979], Loss: 0.5905\n",
            "Epoch [9/20], Step [700/6979], Loss: 0.6027\n",
            "Epoch [9/20], Step [800/6979], Loss: 0.5071\n",
            "Epoch [9/20], Step [900/6979], Loss: 0.5876\n",
            "Epoch [9/20], Step [1000/6979], Loss: 0.5962\n",
            "Epoch [9/20], Step [1100/6979], Loss: 0.4426\n",
            "Epoch [9/20], Step [1200/6979], Loss: 0.4217\n",
            "Epoch [9/20], Step [1300/6979], Loss: 0.5422\n",
            "Epoch [9/20], Step [1400/6979], Loss: 0.5379\n",
            "Epoch [9/20], Step [1500/6979], Loss: 0.5255\n",
            "Epoch [9/20], Step [1600/6979], Loss: 0.6099\n",
            "Epoch [9/20], Step [1700/6979], Loss: 0.4207\n",
            "Epoch [9/20], Step [1800/6979], Loss: 0.3208\n",
            "Epoch [9/20], Step [1900/6979], Loss: 0.6563\n",
            "Epoch [9/20], Step [2000/6979], Loss: 0.7195\n",
            "Epoch [9/20], Step [2100/6979], Loss: 0.5683\n",
            "Epoch [9/20], Step [2200/6979], Loss: 0.4214\n",
            "Epoch [9/20], Step [2300/6979], Loss: 0.6122\n",
            "Epoch [9/20], Step [2400/6979], Loss: 0.5321\n",
            "Epoch [9/20], Step [2500/6979], Loss: 0.5034\n",
            "Epoch [9/20], Step [2600/6979], Loss: 0.4148\n",
            "Epoch [9/20], Step [2700/6979], Loss: 0.4672\n",
            "Epoch [9/20], Step [2800/6979], Loss: 0.3805\n",
            "Epoch [9/20], Step [2900/6979], Loss: 0.4466\n",
            "Epoch [9/20], Step [3000/6979], Loss: 0.5747\n",
            "Epoch [9/20], Step [3100/6979], Loss: 0.5007\n",
            "Epoch [9/20], Step [3200/6979], Loss: 0.4832\n",
            "Epoch [9/20], Step [3300/6979], Loss: 0.4569\n",
            "Epoch [9/20], Step [3400/6979], Loss: 0.3911\n",
            "Epoch [9/20], Step [3500/6979], Loss: 0.4325\n",
            "Epoch [9/20], Step [3600/6979], Loss: 0.5544\n",
            "Epoch [9/20], Step [3700/6979], Loss: 0.3768\n",
            "Epoch [9/20], Step [3800/6979], Loss: 0.5227\n",
            "Epoch [9/20], Step [3900/6979], Loss: 0.4135\n",
            "Epoch [9/20], Step [4000/6979], Loss: 0.4854\n",
            "Epoch [9/20], Step [4100/6979], Loss: 0.5751\n",
            "Epoch [9/20], Step [4200/6979], Loss: 0.5955\n",
            "Epoch [9/20], Step [4300/6979], Loss: 0.5112\n",
            "Epoch [9/20], Step [4400/6979], Loss: 0.6011\n",
            "Epoch [9/20], Step [4500/6979], Loss: 0.4543\n",
            "Epoch [9/20], Step [4600/6979], Loss: 0.4658\n",
            "Epoch [9/20], Step [4700/6979], Loss: 0.4725\n",
            "Epoch [9/20], Step [4800/6979], Loss: 0.4693\n",
            "Epoch [9/20], Step [4900/6979], Loss: 0.5380\n",
            "Epoch [9/20], Step [5000/6979], Loss: 0.3796\n",
            "Epoch [9/20], Step [5100/6979], Loss: 0.2619\n",
            "Epoch [9/20], Step [5200/6979], Loss: 0.4854\n",
            "Epoch [9/20], Step [5300/6979], Loss: 0.6642\n",
            "Epoch [9/20], Step [5400/6979], Loss: 0.5659\n",
            "Epoch [9/20], Step [5500/6979], Loss: 0.6955\n",
            "Epoch [9/20], Step [5600/6979], Loss: 0.5941\n",
            "Epoch [9/20], Step [5700/6979], Loss: 0.4253\n",
            "Epoch [9/20], Step [5800/6979], Loss: 0.2957\n",
            "Epoch [9/20], Step [5900/6979], Loss: 0.5745\n",
            "Epoch [9/20], Step [6000/6979], Loss: 0.5258\n",
            "Epoch [9/20], Step [6100/6979], Loss: 0.4439\n",
            "Epoch [9/20], Step [6200/6979], Loss: 0.4459\n",
            "Epoch [9/20], Step [6300/6979], Loss: 0.7537\n",
            "Epoch [9/20], Step [6400/6979], Loss: 0.3378\n",
            "Epoch [9/20], Step [6500/6979], Loss: 0.5031\n",
            "Epoch [9/20], Step [6600/6979], Loss: 0.5467\n",
            "Epoch [9/20], Step [6700/6979], Loss: 0.3529\n",
            "Epoch [9/20], Step [6800/6979], Loss: 0.5817\n",
            "Epoch [9/20], Step [6900/6979], Loss: 0.4056\n",
            "Epoch [10/20], Step [100/6979], Loss: 0.4049\n",
            "Epoch [10/20], Step [200/6979], Loss: 0.3803\n",
            "Epoch [10/20], Step [300/6979], Loss: 0.3633\n",
            "Epoch [10/20], Step [400/6979], Loss: 0.4790\n",
            "Epoch [10/20], Step [500/6979], Loss: 0.4565\n",
            "Epoch [10/20], Step [600/6979], Loss: 0.5136\n",
            "Epoch [10/20], Step [700/6979], Loss: 0.6797\n",
            "Epoch [10/20], Step [800/6979], Loss: 0.4865\n",
            "Epoch [10/20], Step [900/6979], Loss: 0.4974\n",
            "Epoch [10/20], Step [1000/6979], Loss: 0.4331\n",
            "Epoch [10/20], Step [1100/6979], Loss: 0.5947\n",
            "Epoch [10/20], Step [1200/6979], Loss: 0.5191\n",
            "Epoch [10/20], Step [1300/6979], Loss: 0.4937\n",
            "Epoch [10/20], Step [1400/6979], Loss: 0.4351\n",
            "Epoch [10/20], Step [1500/6979], Loss: 0.5508\n",
            "Epoch [10/20], Step [1600/6979], Loss: 0.4045\n",
            "Epoch [10/20], Step [1700/6979], Loss: 0.4568\n",
            "Epoch [10/20], Step [1800/6979], Loss: 0.4042\n",
            "Epoch [10/20], Step [1900/6979], Loss: 0.4966\n",
            "Epoch [10/20], Step [2000/6979], Loss: 0.4029\n",
            "Epoch [10/20], Step [2100/6979], Loss: 0.4249\n",
            "Epoch [10/20], Step [2200/6979], Loss: 0.5233\n",
            "Epoch [10/20], Step [2300/6979], Loss: 0.4000\n",
            "Epoch [10/20], Step [2400/6979], Loss: 0.7125\n",
            "Epoch [10/20], Step [2500/6979], Loss: 0.6401\n",
            "Epoch [10/20], Step [2600/6979], Loss: 0.5991\n",
            "Epoch [10/20], Step [2700/6979], Loss: 0.3232\n",
            "Epoch [10/20], Step [2800/6979], Loss: 0.5683\n",
            "Epoch [10/20], Step [2900/6979], Loss: 0.3832\n",
            "Epoch [10/20], Step [3000/6979], Loss: 0.4269\n",
            "Epoch [10/20], Step [3100/6979], Loss: 0.5262\n",
            "Epoch [10/20], Step [3200/6979], Loss: 0.4693\n",
            "Epoch [10/20], Step [3300/6979], Loss: 0.7628\n",
            "Epoch [10/20], Step [3400/6979], Loss: 0.4721\n",
            "Epoch [10/20], Step [3500/6979], Loss: 0.3550\n",
            "Epoch [10/20], Step [3600/6979], Loss: 0.4761\n",
            "Epoch [10/20], Step [3700/6979], Loss: 0.4465\n",
            "Epoch [10/20], Step [3800/6979], Loss: 0.4967\n",
            "Epoch [10/20], Step [3900/6979], Loss: 0.3943\n",
            "Epoch [10/20], Step [4000/6979], Loss: 0.5746\n",
            "Epoch [10/20], Step [4100/6979], Loss: 0.4395\n",
            "Epoch [10/20], Step [4200/6979], Loss: 0.3054\n",
            "Epoch [10/20], Step [4300/6979], Loss: 0.3743\n",
            "Epoch [10/20], Step [4400/6979], Loss: 0.5618\n",
            "Epoch [10/20], Step [4500/6979], Loss: 0.4254\n",
            "Epoch [10/20], Step [4600/6979], Loss: 0.3857\n",
            "Epoch [10/20], Step [4700/6979], Loss: 0.4832\n",
            "Epoch [10/20], Step [4800/6979], Loss: 0.6020\n",
            "Epoch [10/20], Step [4900/6979], Loss: 0.5899\n",
            "Epoch [10/20], Step [5000/6979], Loss: 0.3973\n",
            "Epoch [10/20], Step [5100/6979], Loss: 0.5077\n",
            "Epoch [10/20], Step [5200/6979], Loss: 0.3382\n",
            "Epoch [10/20], Step [5300/6979], Loss: 0.5621\n",
            "Epoch [10/20], Step [5400/6979], Loss: 0.3961\n",
            "Epoch [10/20], Step [5500/6979], Loss: 0.6774\n",
            "Epoch [10/20], Step [5600/6979], Loss: 0.5616\n",
            "Epoch [10/20], Step [5700/6979], Loss: 0.6360\n",
            "Epoch [10/20], Step [5800/6979], Loss: 0.3630\n",
            "Epoch [10/20], Step [5900/6979], Loss: 0.4042\n",
            "Epoch [10/20], Step [6000/6979], Loss: 0.5079\n",
            "Epoch [10/20], Step [6100/6979], Loss: 0.4993\n",
            "Epoch [10/20], Step [6200/6979], Loss: 0.3808\n",
            "Epoch [10/20], Step [6300/6979], Loss: 0.3507\n",
            "Epoch [10/20], Step [6400/6979], Loss: 0.3642\n",
            "Epoch [10/20], Step [6500/6979], Loss: 0.5445\n",
            "Epoch [10/20], Step [6600/6979], Loss: 0.5114\n",
            "Epoch [10/20], Step [6700/6979], Loss: 0.5654\n",
            "Epoch [10/20], Step [6800/6979], Loss: 0.6238\n",
            "Epoch [10/20], Step [6900/6979], Loss: 0.3217\n",
            "Epoch [11/20], Step [100/6979], Loss: 0.5092\n",
            "Epoch [11/20], Step [200/6979], Loss: 0.5427\n",
            "Epoch [11/20], Step [300/6979], Loss: 0.3787\n",
            "Epoch [11/20], Step [400/6979], Loss: 0.5658\n",
            "Epoch [11/20], Step [500/6979], Loss: 0.5747\n",
            "Epoch [11/20], Step [600/6979], Loss: 0.4506\n",
            "Epoch [11/20], Step [700/6979], Loss: 0.3966\n",
            "Epoch [11/20], Step [800/6979], Loss: 0.4159\n",
            "Epoch [11/20], Step [900/6979], Loss: 0.5138\n",
            "Epoch [11/20], Step [1000/6979], Loss: 0.4937\n",
            "Epoch [11/20], Step [1100/6979], Loss: 0.4980\n",
            "Epoch [11/20], Step [1200/6979], Loss: 0.5601\n",
            "Epoch [11/20], Step [1300/6979], Loss: 0.6868\n",
            "Epoch [11/20], Step [1400/6979], Loss: 0.4533\n",
            "Epoch [11/20], Step [1500/6979], Loss: 0.6067\n",
            "Epoch [11/20], Step [1600/6979], Loss: 0.3663\n",
            "Epoch [11/20], Step [1700/6979], Loss: 0.4407\n",
            "Epoch [11/20], Step [1800/6979], Loss: 0.3804\n",
            "Epoch [11/20], Step [1900/6979], Loss: 0.4283\n",
            "Epoch [11/20], Step [2000/6979], Loss: 0.4892\n",
            "Epoch [11/20], Step [2100/6979], Loss: 0.4285\n",
            "Epoch [11/20], Step [2200/6979], Loss: 0.4824\n",
            "Epoch [11/20], Step [2300/6979], Loss: 0.6124\n",
            "Epoch [11/20], Step [2400/6979], Loss: 0.3802\n",
            "Epoch [11/20], Step [2500/6979], Loss: 0.4048\n",
            "Epoch [11/20], Step [2600/6979], Loss: 0.6012\n",
            "Epoch [11/20], Step [2700/6979], Loss: 0.3750\n",
            "Epoch [11/20], Step [2800/6979], Loss: 0.5252\n",
            "Epoch [11/20], Step [2900/6979], Loss: 0.3494\n",
            "Epoch [11/20], Step [3000/6979], Loss: 0.4700\n",
            "Epoch [11/20], Step [3100/6979], Loss: 0.4522\n",
            "Epoch [11/20], Step [3200/6979], Loss: 0.4171\n",
            "Epoch [11/20], Step [3300/6979], Loss: 0.4777\n",
            "Epoch [11/20], Step [3400/6979], Loss: 0.5503\n",
            "Epoch [11/20], Step [3500/6979], Loss: 0.4740\n",
            "Epoch [11/20], Step [3600/6979], Loss: 0.4254\n",
            "Epoch [11/20], Step [3700/6979], Loss: 0.6131\n",
            "Epoch [11/20], Step [3800/6979], Loss: 0.5138\n",
            "Epoch [11/20], Step [3900/6979], Loss: 0.4117\n",
            "Epoch [11/20], Step [4000/6979], Loss: 0.3453\n",
            "Epoch [11/20], Step [4100/6979], Loss: 0.4189\n",
            "Epoch [11/20], Step [4200/6979], Loss: 0.2944\n",
            "Epoch [11/20], Step [4300/6979], Loss: 0.4852\n",
            "Epoch [11/20], Step [4400/6979], Loss: 0.4586\n",
            "Epoch [11/20], Step [4500/6979], Loss: 0.5077\n",
            "Epoch [11/20], Step [4600/6979], Loss: 0.5579\n",
            "Epoch [11/20], Step [4700/6979], Loss: 0.4842\n",
            "Epoch [11/20], Step [4800/6979], Loss: 0.4265\n",
            "Epoch [11/20], Step [4900/6979], Loss: 0.3515\n",
            "Epoch [11/20], Step [5000/6979], Loss: 0.3888\n",
            "Epoch [11/20], Step [5100/6979], Loss: 0.6308\n",
            "Epoch [11/20], Step [5200/6979], Loss: 0.6656\n",
            "Epoch [11/20], Step [5300/6979], Loss: 0.5850\n",
            "Epoch [11/20], Step [5400/6979], Loss: 0.4685\n",
            "Epoch [11/20], Step [5500/6979], Loss: 0.4379\n",
            "Epoch [11/20], Step [5600/6979], Loss: 0.6517\n",
            "Epoch [11/20], Step [5700/6979], Loss: 0.4398\n",
            "Epoch [11/20], Step [5800/6979], Loss: 0.4782\n",
            "Epoch [11/20], Step [5900/6979], Loss: 0.4639\n",
            "Epoch [11/20], Step [6000/6979], Loss: 0.5247\n",
            "Epoch [11/20], Step [6100/6979], Loss: 0.3192\n",
            "Epoch [11/20], Step [6200/6979], Loss: 0.5886\n",
            "Epoch [11/20], Step [6300/6979], Loss: 0.4420\n",
            "Epoch [11/20], Step [6400/6979], Loss: 0.2986\n",
            "Epoch [11/20], Step [6500/6979], Loss: 0.3425\n",
            "Epoch [11/20], Step [6600/6979], Loss: 0.4015\n",
            "Epoch [11/20], Step [6700/6979], Loss: 0.5679\n",
            "Epoch [11/20], Step [6800/6979], Loss: 0.4319\n",
            "Epoch [11/20], Step [6900/6979], Loss: 0.4411\n",
            "Epoch [12/20], Step [100/6979], Loss: 0.3610\n",
            "Epoch [12/20], Step [200/6979], Loss: 0.5411\n",
            "Epoch [12/20], Step [300/6979], Loss: 0.3770\n",
            "Epoch [12/20], Step [400/6979], Loss: 0.3098\n",
            "Epoch [12/20], Step [500/6979], Loss: 0.5092\n",
            "Epoch [12/20], Step [600/6979], Loss: 0.4421\n",
            "Epoch [12/20], Step [700/6979], Loss: 0.2947\n",
            "Epoch [12/20], Step [800/6979], Loss: 0.3521\n",
            "Epoch [12/20], Step [900/6979], Loss: 0.5286\n",
            "Epoch [12/20], Step [1000/6979], Loss: 0.6322\n",
            "Epoch [12/20], Step [1100/6979], Loss: 0.4854\n",
            "Epoch [12/20], Step [1200/6979], Loss: 0.4699\n",
            "Epoch [12/20], Step [1300/6979], Loss: 0.4151\n",
            "Epoch [12/20], Step [1400/6979], Loss: 0.4263\n",
            "Epoch [12/20], Step [1500/6979], Loss: 0.5820\n",
            "Epoch [12/20], Step [1600/6979], Loss: 0.3909\n",
            "Epoch [12/20], Step [1700/6979], Loss: 0.5312\n",
            "Epoch [12/20], Step [1800/6979], Loss: 0.5068\n",
            "Epoch [12/20], Step [1900/6979], Loss: 0.3009\n",
            "Epoch [12/20], Step [2000/6979], Loss: 0.5455\n",
            "Epoch [12/20], Step [2100/6979], Loss: 0.4076\n",
            "Epoch [12/20], Step [2200/6979], Loss: 0.3220\n",
            "Epoch [12/20], Step [2300/6979], Loss: 0.5048\n",
            "Epoch [12/20], Step [2400/6979], Loss: 0.3626\n",
            "Epoch [12/20], Step [2500/6979], Loss: 0.4773\n",
            "Epoch [12/20], Step [2600/6979], Loss: 0.5811\n",
            "Epoch [12/20], Step [2700/6979], Loss: 0.3839\n",
            "Epoch [12/20], Step [2800/6979], Loss: 0.5849\n",
            "Epoch [12/20], Step [2900/6979], Loss: 0.5755\n",
            "Epoch [12/20], Step [3000/6979], Loss: 0.5003\n",
            "Epoch [12/20], Step [3100/6979], Loss: 0.5800\n",
            "Epoch [12/20], Step [3200/6979], Loss: 0.4782\n",
            "Epoch [12/20], Step [3300/6979], Loss: 0.5041\n",
            "Epoch [12/20], Step [3400/6979], Loss: 0.4630\n",
            "Epoch [12/20], Step [3500/6979], Loss: 0.5076\n",
            "Epoch [12/20], Step [3600/6979], Loss: 0.4643\n",
            "Epoch [12/20], Step [3700/6979], Loss: 0.5724\n",
            "Epoch [12/20], Step [3800/6979], Loss: 0.3278\n",
            "Epoch [12/20], Step [3900/6979], Loss: 0.3937\n",
            "Epoch [12/20], Step [4000/6979], Loss: 0.4194\n",
            "Epoch [12/20], Step [4100/6979], Loss: 0.5921\n",
            "Epoch [12/20], Step [4200/6979], Loss: 0.3657\n",
            "Epoch [12/20], Step [4300/6979], Loss: 0.3615\n",
            "Epoch [12/20], Step [4400/6979], Loss: 0.6577\n",
            "Epoch [12/20], Step [4500/6979], Loss: 0.4094\n",
            "Epoch [12/20], Step [4600/6979], Loss: 0.5033\n",
            "Epoch [12/20], Step [4700/6979], Loss: 0.3957\n",
            "Epoch [12/20], Step [4800/6979], Loss: 0.3130\n",
            "Epoch [12/20], Step [4900/6979], Loss: 0.4526\n",
            "Epoch [12/20], Step [5000/6979], Loss: 0.3835\n",
            "Epoch [12/20], Step [5100/6979], Loss: 0.4605\n",
            "Epoch [12/20], Step [5200/6979], Loss: 0.5739\n",
            "Epoch [12/20], Step [5300/6979], Loss: 0.7360\n",
            "Epoch [12/20], Step [5400/6979], Loss: 0.5609\n",
            "Epoch [12/20], Step [5500/6979], Loss: 0.4442\n",
            "Epoch [12/20], Step [5600/6979], Loss: 0.4508\n",
            "Epoch [12/20], Step [5700/6979], Loss: 0.3723\n",
            "Epoch [12/20], Step [5800/6979], Loss: 0.4322\n",
            "Epoch [12/20], Step [5900/6979], Loss: 0.5297\n",
            "Epoch [12/20], Step [6000/6979], Loss: 0.5231\n",
            "Epoch [12/20], Step [6100/6979], Loss: 0.4208\n",
            "Epoch [12/20], Step [6200/6979], Loss: 0.3551\n",
            "Epoch [12/20], Step [6300/6979], Loss: 0.5258\n",
            "Epoch [12/20], Step [6400/6979], Loss: 0.3893\n",
            "Epoch [12/20], Step [6500/6979], Loss: 0.3253\n",
            "Epoch [12/20], Step [6600/6979], Loss: 0.3988\n",
            "Epoch [12/20], Step [6700/6979], Loss: 0.3909\n",
            "Epoch [12/20], Step [6800/6979], Loss: 0.3280\n",
            "Epoch [12/20], Step [6900/6979], Loss: 0.6355\n",
            "Epoch [13/20], Step [100/6979], Loss: 0.4814\n",
            "Epoch [13/20], Step [200/6979], Loss: 0.4547\n",
            "Epoch [13/20], Step [300/6979], Loss: 0.4400\n",
            "Epoch [13/20], Step [400/6979], Loss: 0.3428\n",
            "Epoch [13/20], Step [500/6979], Loss: 0.5725\n",
            "Epoch [13/20], Step [600/6979], Loss: 0.5580\n",
            "Epoch [13/20], Step [700/6979], Loss: 0.4606\n",
            "Epoch [13/20], Step [800/6979], Loss: 0.4972\n",
            "Epoch [13/20], Step [900/6979], Loss: 0.2991\n",
            "Epoch [13/20], Step [1000/6979], Loss: 0.3854\n",
            "Epoch [13/20], Step [1100/6979], Loss: 0.5523\n",
            "Epoch [13/20], Step [1200/6979], Loss: 0.3980\n",
            "Epoch [13/20], Step [1300/6979], Loss: 0.4553\n",
            "Epoch [13/20], Step [1400/6979], Loss: 0.3261\n",
            "Epoch [13/20], Step [1500/6979], Loss: 0.2992\n",
            "Epoch [13/20], Step [1600/6979], Loss: 0.4410\n",
            "Epoch [13/20], Step [1700/6979], Loss: 0.4978\n",
            "Epoch [13/20], Step [1800/6979], Loss: 0.4575\n",
            "Epoch [13/20], Step [1900/6979], Loss: 0.3388\n",
            "Epoch [13/20], Step [2000/6979], Loss: 0.6191\n",
            "Epoch [13/20], Step [2100/6979], Loss: 0.4408\n",
            "Epoch [13/20], Step [2200/6979], Loss: 0.4268\n",
            "Epoch [13/20], Step [2300/6979], Loss: 0.5730\n",
            "Epoch [13/20], Step [2400/6979], Loss: 0.4110\n",
            "Epoch [13/20], Step [2500/6979], Loss: 0.3446\n",
            "Epoch [13/20], Step [2600/6979], Loss: 0.4278\n",
            "Epoch [13/20], Step [2700/6979], Loss: 0.3531\n",
            "Epoch [13/20], Step [2800/6979], Loss: 0.3305\n",
            "Epoch [13/20], Step [2900/6979], Loss: 0.3447\n",
            "Epoch [13/20], Step [3000/6979], Loss: 0.4820\n",
            "Epoch [13/20], Step [3100/6979], Loss: 0.4732\n",
            "Epoch [13/20], Step [3200/6979], Loss: 0.3853\n",
            "Epoch [13/20], Step [3300/6979], Loss: 0.3232\n",
            "Epoch [13/20], Step [3400/6979], Loss: 0.3413\n",
            "Epoch [13/20], Step [3500/6979], Loss: 0.3325\n",
            "Epoch [13/20], Step [3600/6979], Loss: 0.4590\n",
            "Epoch [13/20], Step [3700/6979], Loss: 0.5576\n",
            "Epoch [13/20], Step [3800/6979], Loss: 0.4540\n",
            "Epoch [13/20], Step [3900/6979], Loss: 0.4689\n",
            "Epoch [13/20], Step [4000/6979], Loss: 0.3793\n",
            "Epoch [13/20], Step [4100/6979], Loss: 0.4428\n",
            "Epoch [13/20], Step [4200/6979], Loss: 0.4561\n",
            "Epoch [13/20], Step [4300/6979], Loss: 0.6858\n",
            "Epoch [13/20], Step [4400/6979], Loss: 0.4883\n",
            "Epoch [13/20], Step [4500/6979], Loss: 0.4009\n",
            "Epoch [13/20], Step [4600/6979], Loss: 0.3097\n",
            "Epoch [13/20], Step [4700/6979], Loss: 0.4345\n",
            "Epoch [13/20], Step [4800/6979], Loss: 0.3394\n",
            "Epoch [13/20], Step [4900/6979], Loss: 0.3189\n",
            "Epoch [13/20], Step [5000/6979], Loss: 0.3984\n",
            "Epoch [13/20], Step [5100/6979], Loss: 0.4597\n",
            "Epoch [13/20], Step [5200/6979], Loss: 0.3994\n",
            "Epoch [13/20], Step [5300/6979], Loss: 0.4603\n",
            "Epoch [13/20], Step [5400/6979], Loss: 0.4942\n",
            "Epoch [13/20], Step [5500/6979], Loss: 0.4497\n",
            "Epoch [13/20], Step [5600/6979], Loss: 0.5369\n",
            "Epoch [13/20], Step [5700/6979], Loss: 0.3703\n",
            "Epoch [13/20], Step [5800/6979], Loss: 0.4774\n",
            "Epoch [13/20], Step [5900/6979], Loss: 0.4801\n",
            "Epoch [13/20], Step [6000/6979], Loss: 0.3745\n",
            "Epoch [13/20], Step [6100/6979], Loss: 0.3679\n",
            "Epoch [13/20], Step [6200/6979], Loss: 0.3338\n",
            "Epoch [13/20], Step [6300/6979], Loss: 0.2859\n",
            "Epoch [13/20], Step [6400/6979], Loss: 0.4081\n",
            "Epoch [13/20], Step [6500/6979], Loss: 0.5379\n",
            "Epoch [13/20], Step [6600/6979], Loss: 0.4777\n",
            "Epoch [13/20], Step [6700/6979], Loss: 0.5740\n",
            "Epoch [13/20], Step [6800/6979], Loss: 0.4233\n",
            "Epoch [13/20], Step [6900/6979], Loss: 0.5209\n",
            "Epoch [14/20], Step [100/6979], Loss: 0.3501\n",
            "Epoch [14/20], Step [200/6979], Loss: 0.4115\n",
            "Epoch [14/20], Step [300/6979], Loss: 0.5266\n",
            "Epoch [14/20], Step [400/6979], Loss: 0.5583\n",
            "Epoch [14/20], Step [500/6979], Loss: 0.3369\n",
            "Epoch [14/20], Step [600/6979], Loss: 0.5615\n",
            "Epoch [14/20], Step [700/6979], Loss: 0.3872\n",
            "Epoch [14/20], Step [800/6979], Loss: 0.6632\n",
            "Epoch [14/20], Step [900/6979], Loss: 0.3785\n",
            "Epoch [14/20], Step [1000/6979], Loss: 0.3336\n",
            "Epoch [14/20], Step [1100/6979], Loss: 0.3832\n",
            "Epoch [14/20], Step [1200/6979], Loss: 0.3997\n",
            "Epoch [14/20], Step [1300/6979], Loss: 0.3994\n",
            "Epoch [14/20], Step [1400/6979], Loss: 0.4288\n",
            "Epoch [14/20], Step [1500/6979], Loss: 0.5055\n",
            "Epoch [14/20], Step [1600/6979], Loss: 0.5068\n",
            "Epoch [14/20], Step [1700/6979], Loss: 0.3852\n",
            "Epoch [14/20], Step [1800/6979], Loss: 0.3313\n",
            "Epoch [14/20], Step [1900/6979], Loss: 0.5020\n",
            "Epoch [14/20], Step [2000/6979], Loss: 0.5601\n",
            "Epoch [14/20], Step [2100/6979], Loss: 0.3353\n",
            "Epoch [14/20], Step [2200/6979], Loss: 0.3525\n",
            "Epoch [14/20], Step [2300/6979], Loss: 0.4649\n",
            "Epoch [14/20], Step [2400/6979], Loss: 0.3713\n",
            "Epoch [14/20], Step [2500/6979], Loss: 0.5387\n",
            "Epoch [14/20], Step [2600/6979], Loss: 0.4262\n",
            "Epoch [14/20], Step [2700/6979], Loss: 0.3921\n",
            "Epoch [14/20], Step [2800/6979], Loss: 0.6235\n",
            "Epoch [14/20], Step [2900/6979], Loss: 0.4083\n",
            "Epoch [14/20], Step [3000/6979], Loss: 0.6431\n",
            "Epoch [14/20], Step [3100/6979], Loss: 0.3850\n",
            "Epoch [14/20], Step [3200/6979], Loss: 0.6258\n",
            "Epoch [14/20], Step [3300/6979], Loss: 0.4089\n",
            "Epoch [14/20], Step [3400/6979], Loss: 0.6258\n",
            "Epoch [14/20], Step [3500/6979], Loss: 0.3244\n",
            "Epoch [14/20], Step [3600/6979], Loss: 0.4016\n",
            "Epoch [14/20], Step [3700/6979], Loss: 0.4147\n",
            "Epoch [14/20], Step [3800/6979], Loss: 0.5171\n",
            "Epoch [14/20], Step [3900/6979], Loss: 0.5407\n",
            "Epoch [14/20], Step [4000/6979], Loss: 0.5369\n",
            "Epoch [14/20], Step [4100/6979], Loss: 0.3896\n",
            "Epoch [14/20], Step [4200/6979], Loss: 0.2907\n",
            "Epoch [14/20], Step [4300/6979], Loss: 0.3415\n",
            "Epoch [14/20], Step [4400/6979], Loss: 0.5119\n",
            "Epoch [14/20], Step [4500/6979], Loss: 0.4579\n",
            "Epoch [14/20], Step [4600/6979], Loss: 0.4830\n",
            "Epoch [14/20], Step [4700/6979], Loss: 0.3149\n",
            "Epoch [14/20], Step [4800/6979], Loss: 0.4883\n",
            "Epoch [14/20], Step [4900/6979], Loss: 0.5406\n",
            "Epoch [14/20], Step [5000/6979], Loss: 0.4845\n",
            "Epoch [14/20], Step [5100/6979], Loss: 0.3273\n",
            "Epoch [14/20], Step [5200/6979], Loss: 0.6117\n",
            "Epoch [14/20], Step [5300/6979], Loss: 0.4239\n",
            "Epoch [14/20], Step [5400/6979], Loss: 0.3296\n",
            "Epoch [14/20], Step [5500/6979], Loss: 0.4677\n",
            "Epoch [14/20], Step [5600/6979], Loss: 0.3278\n",
            "Epoch [14/20], Step [5700/6979], Loss: 0.4839\n",
            "Epoch [14/20], Step [5800/6979], Loss: 0.5918\n",
            "Epoch [14/20], Step [5900/6979], Loss: 0.3699\n",
            "Epoch [14/20], Step [6000/6979], Loss: 0.4654\n",
            "Epoch [14/20], Step [6100/6979], Loss: 0.3555\n",
            "Epoch [14/20], Step [6200/6979], Loss: 0.2927\n",
            "Epoch [14/20], Step [6300/6979], Loss: 0.5215\n",
            "Epoch [14/20], Step [6400/6979], Loss: 0.3735\n",
            "Epoch [14/20], Step [6500/6979], Loss: 0.3926\n",
            "Epoch [14/20], Step [6600/6979], Loss: 0.4081\n",
            "Epoch [14/20], Step [6700/6979], Loss: 0.6200\n",
            "Epoch [14/20], Step [6800/6979], Loss: 0.4176\n",
            "Epoch [14/20], Step [6900/6979], Loss: 0.5287\n",
            "Epoch [15/20], Step [100/6979], Loss: 0.5046\n",
            "Epoch [15/20], Step [200/6979], Loss: 0.5913\n",
            "Epoch [15/20], Step [300/6979], Loss: 0.2978\n",
            "Epoch [15/20], Step [400/6979], Loss: 0.5475\n",
            "Epoch [15/20], Step [500/6979], Loss: 0.4017\n",
            "Epoch [15/20], Step [600/6979], Loss: 0.6339\n",
            "Epoch [15/20], Step [700/6979], Loss: 0.4749\n",
            "Epoch [15/20], Step [800/6979], Loss: 0.4367\n",
            "Epoch [15/20], Step [900/6979], Loss: 0.3786\n",
            "Epoch [15/20], Step [1000/6979], Loss: 0.5500\n",
            "Epoch [15/20], Step [1100/6979], Loss: 0.5232\n",
            "Epoch [15/20], Step [1200/6979], Loss: 0.5673\n",
            "Epoch [15/20], Step [1300/6979], Loss: 0.3490\n",
            "Epoch [15/20], Step [1400/6979], Loss: 0.4797\n",
            "Epoch [15/20], Step [1500/6979], Loss: 0.4434\n",
            "Epoch [15/20], Step [1600/6979], Loss: 0.6035\n",
            "Epoch [15/20], Step [1700/6979], Loss: 0.5115\n",
            "Epoch [15/20], Step [1800/6979], Loss: 0.3863\n",
            "Epoch [15/20], Step [1900/6979], Loss: 0.4597\n",
            "Epoch [15/20], Step [2000/6979], Loss: 0.4407\n",
            "Epoch [15/20], Step [2100/6979], Loss: 0.4911\n",
            "Epoch [15/20], Step [2200/6979], Loss: 0.4175\n",
            "Epoch [15/20], Step [2300/6979], Loss: 0.4508\n",
            "Epoch [15/20], Step [2400/6979], Loss: 0.3862\n",
            "Epoch [15/20], Step [2500/6979], Loss: 0.6646\n",
            "Epoch [15/20], Step [2600/6979], Loss: 0.4845\n",
            "Epoch [15/20], Step [2700/6979], Loss: 0.3979\n",
            "Epoch [15/20], Step [2800/6979], Loss: 0.4615\n",
            "Epoch [15/20], Step [2900/6979], Loss: 0.3164\n",
            "Epoch [15/20], Step [3000/6979], Loss: 0.4970\n",
            "Epoch [15/20], Step [3100/6979], Loss: 0.4448\n",
            "Epoch [15/20], Step [3200/6979], Loss: 0.3477\n",
            "Epoch [15/20], Step [3300/6979], Loss: 0.4633\n",
            "Epoch [15/20], Step [3400/6979], Loss: 0.6068\n",
            "Epoch [15/20], Step [3500/6979], Loss: 0.4877\n",
            "Epoch [15/20], Step [3600/6979], Loss: 0.6723\n",
            "Epoch [15/20], Step [3700/6979], Loss: 0.4427\n",
            "Epoch [15/20], Step [3800/6979], Loss: 0.3632\n",
            "Epoch [15/20], Step [3900/6979], Loss: 0.4463\n",
            "Epoch [15/20], Step [4000/6979], Loss: 0.3645\n",
            "Epoch [15/20], Step [4100/6979], Loss: 0.6075\n",
            "Epoch [15/20], Step [4200/6979], Loss: 0.3632\n",
            "Epoch [15/20], Step [4300/6979], Loss: 0.4920\n",
            "Epoch [15/20], Step [4400/6979], Loss: 0.4002\n",
            "Epoch [15/20], Step [4500/6979], Loss: 0.4003\n",
            "Epoch [15/20], Step [4600/6979], Loss: 0.4855\n",
            "Epoch [15/20], Step [4700/6979], Loss: 0.4422\n",
            "Epoch [15/20], Step [4800/6979], Loss: 0.4367\n",
            "Epoch [15/20], Step [4900/6979], Loss: 0.4407\n",
            "Epoch [15/20], Step [5000/6979], Loss: 0.6921\n",
            "Epoch [15/20], Step [5100/6979], Loss: 0.4130\n",
            "Epoch [15/20], Step [5200/6979], Loss: 0.4447\n",
            "Epoch [15/20], Step [5300/6979], Loss: 0.3723\n",
            "Epoch [15/20], Step [5400/6979], Loss: 0.5970\n",
            "Epoch [15/20], Step [5500/6979], Loss: 0.5126\n",
            "Epoch [15/20], Step [5600/6979], Loss: 0.6028\n",
            "Epoch [15/20], Step [5700/6979], Loss: 0.4550\n",
            "Epoch [15/20], Step [5800/6979], Loss: 0.3789\n",
            "Epoch [15/20], Step [5900/6979], Loss: 0.5558\n",
            "Epoch [15/20], Step [6000/6979], Loss: 0.6522\n",
            "Epoch [15/20], Step [6100/6979], Loss: 0.3275\n",
            "Epoch [15/20], Step [6200/6979], Loss: 0.2769\n",
            "Epoch [15/20], Step [6300/6979], Loss: 0.3090\n",
            "Epoch [15/20], Step [6400/6979], Loss: 0.4619\n",
            "Epoch [15/20], Step [6500/6979], Loss: 0.5225\n",
            "Epoch [15/20], Step [6600/6979], Loss: 0.5352\n",
            "Epoch [15/20], Step [6700/6979], Loss: 0.3899\n",
            "Epoch [15/20], Step [6800/6979], Loss: 0.5491\n",
            "Epoch [15/20], Step [6900/6979], Loss: 0.5311\n",
            "Epoch [16/20], Step [100/6979], Loss: 0.4623\n",
            "Epoch [16/20], Step [200/6979], Loss: 0.3544\n",
            "Epoch [16/20], Step [300/6979], Loss: 0.6012\n",
            "Epoch [16/20], Step [400/6979], Loss: 0.5560\n",
            "Epoch [16/20], Step [500/6979], Loss: 0.3483\n",
            "Epoch [16/20], Step [600/6979], Loss: 0.3653\n",
            "Epoch [16/20], Step [700/6979], Loss: 0.3124\n",
            "Epoch [16/20], Step [800/6979], Loss: 0.4617\n",
            "Epoch [16/20], Step [900/6979], Loss: 0.5155\n",
            "Epoch [16/20], Step [1000/6979], Loss: 0.4094\n",
            "Epoch [16/20], Step [1100/6979], Loss: 0.4812\n",
            "Epoch [16/20], Step [1200/6979], Loss: 0.4732\n",
            "Epoch [16/20], Step [1300/6979], Loss: 0.2922\n",
            "Epoch [16/20], Step [1400/6979], Loss: 0.2984\n",
            "Epoch [16/20], Step [1500/6979], Loss: 0.4023\n",
            "Epoch [16/20], Step [1600/6979], Loss: 0.5186\n",
            "Epoch [16/20], Step [1700/6979], Loss: 0.4839\n",
            "Epoch [16/20], Step [1800/6979], Loss: 0.5552\n",
            "Epoch [16/20], Step [1900/6979], Loss: 0.6046\n",
            "Epoch [16/20], Step [2000/6979], Loss: 0.4171\n",
            "Epoch [16/20], Step [2100/6979], Loss: 0.4359\n",
            "Epoch [16/20], Step [2200/6979], Loss: 0.3787\n",
            "Epoch [16/20], Step [2300/6979], Loss: 0.3969\n",
            "Epoch [16/20], Step [2400/6979], Loss: 0.3303\n",
            "Epoch [16/20], Step [2500/6979], Loss: 0.2553\n",
            "Epoch [16/20], Step [2600/6979], Loss: 0.4832\n",
            "Epoch [16/20], Step [2700/6979], Loss: 0.4213\n",
            "Epoch [16/20], Step [2800/6979], Loss: 0.3916\n",
            "Epoch [16/20], Step [2900/6979], Loss: 0.5962\n",
            "Epoch [16/20], Step [3000/6979], Loss: 0.4834\n",
            "Epoch [16/20], Step [3100/6979], Loss: 0.2945\n",
            "Epoch [16/20], Step [3200/6979], Loss: 0.4686\n",
            "Epoch [16/20], Step [3300/6979], Loss: 0.3380\n",
            "Epoch [16/20], Step [3400/6979], Loss: 0.4676\n",
            "Epoch [16/20], Step [3500/6979], Loss: 0.3921\n",
            "Epoch [16/20], Step [3600/6979], Loss: 0.4484\n",
            "Epoch [16/20], Step [3700/6979], Loss: 0.3869\n",
            "Epoch [16/20], Step [3800/6979], Loss: 0.4626\n",
            "Epoch [16/20], Step [3900/6979], Loss: 0.4223\n",
            "Epoch [16/20], Step [4000/6979], Loss: 0.3943\n",
            "Epoch [16/20], Step [4100/6979], Loss: 0.6627\n",
            "Epoch [16/20], Step [4200/6979], Loss: 0.3828\n",
            "Epoch [16/20], Step [4300/6979], Loss: 0.4043\n",
            "Epoch [16/20], Step [4400/6979], Loss: 0.4212\n",
            "Epoch [16/20], Step [4500/6979], Loss: 0.4358\n",
            "Epoch [16/20], Step [4600/6979], Loss: 0.5432\n",
            "Epoch [16/20], Step [4700/6979], Loss: 0.5401\n",
            "Epoch [16/20], Step [4800/6979], Loss: 0.4410\n",
            "Epoch [16/20], Step [4900/6979], Loss: 0.6071\n",
            "Epoch [16/20], Step [5000/6979], Loss: 0.3579\n",
            "Epoch [16/20], Step [5100/6979], Loss: 0.5530\n",
            "Epoch [16/20], Step [5200/6979], Loss: 0.4551\n",
            "Epoch [16/20], Step [5300/6979], Loss: 0.3222\n",
            "Epoch [16/20], Step [5400/6979], Loss: 0.5172\n",
            "Epoch [16/20], Step [5500/6979], Loss: 0.3873\n",
            "Epoch [16/20], Step [5600/6979], Loss: 0.5341\n",
            "Epoch [16/20], Step [5700/6979], Loss: 0.4871\n",
            "Epoch [16/20], Step [5800/6979], Loss: 0.4995\n",
            "Epoch [16/20], Step [5900/6979], Loss: 0.3445\n",
            "Epoch [16/20], Step [6000/6979], Loss: 0.3320\n",
            "Epoch [16/20], Step [6100/6979], Loss: 0.4077\n",
            "Epoch [16/20], Step [6200/6979], Loss: 0.4572\n",
            "Epoch [16/20], Step [6300/6979], Loss: 0.3650\n",
            "Epoch [16/20], Step [6400/6979], Loss: 0.3747\n",
            "Epoch [16/20], Step [6500/6979], Loss: 0.4261\n",
            "Epoch [16/20], Step [6600/6979], Loss: 0.4515\n",
            "Epoch [16/20], Step [6700/6979], Loss: 0.4860\n",
            "Epoch [16/20], Step [6800/6979], Loss: 0.5469\n",
            "Epoch [16/20], Step [6900/6979], Loss: 0.3683\n",
            "Epoch [17/20], Step [100/6979], Loss: 0.3904\n",
            "Epoch [17/20], Step [200/6979], Loss: 0.3173\n",
            "Epoch [17/20], Step [300/6979], Loss: 0.4072\n",
            "Epoch [17/20], Step [400/6979], Loss: 0.2957\n",
            "Epoch [17/20], Step [500/6979], Loss: 0.3754\n",
            "Epoch [17/20], Step [600/6979], Loss: 0.3952\n",
            "Epoch [17/20], Step [700/6979], Loss: 0.3903\n",
            "Epoch [17/20], Step [800/6979], Loss: 0.6106\n",
            "Epoch [17/20], Step [900/6979], Loss: 0.6591\n",
            "Epoch [17/20], Step [1000/6979], Loss: 0.6941\n",
            "Epoch [17/20], Step [1100/6979], Loss: 0.4485\n",
            "Epoch [17/20], Step [1200/6979], Loss: 0.4312\n",
            "Epoch [17/20], Step [1300/6979], Loss: 0.4000\n",
            "Epoch [17/20], Step [1400/6979], Loss: 0.4987\n",
            "Epoch [17/20], Step [1500/6979], Loss: 0.3189\n",
            "Epoch [17/20], Step [1600/6979], Loss: 0.4129\n",
            "Epoch [17/20], Step [1700/6979], Loss: 0.2707\n",
            "Epoch [17/20], Step [1800/6979], Loss: 0.7930\n",
            "Epoch [17/20], Step [1900/6979], Loss: 0.5578\n",
            "Epoch [17/20], Step [2000/6979], Loss: 0.4880\n",
            "Epoch [17/20], Step [2100/6979], Loss: 0.4967\n",
            "Epoch [17/20], Step [2200/6979], Loss: 0.5525\n",
            "Epoch [17/20], Step [2300/6979], Loss: 0.3437\n",
            "Epoch [17/20], Step [2400/6979], Loss: 0.3760\n",
            "Epoch [17/20], Step [2500/6979], Loss: 0.3821\n",
            "Epoch [17/20], Step [2600/6979], Loss: 0.3684\n",
            "Epoch [17/20], Step [2700/6979], Loss: 0.5009\n",
            "Epoch [17/20], Step [2800/6979], Loss: 0.5900\n",
            "Epoch [17/20], Step [2900/6979], Loss: 0.5573\n",
            "Epoch [17/20], Step [3000/6979], Loss: 0.5907\n",
            "Epoch [17/20], Step [3100/6979], Loss: 0.4098\n",
            "Epoch [17/20], Step [3200/6979], Loss: 0.4614\n",
            "Epoch [17/20], Step [3300/6979], Loss: 0.5129\n",
            "Epoch [17/20], Step [3400/6979], Loss: 0.4515\n",
            "Epoch [17/20], Step [3500/6979], Loss: 0.5688\n",
            "Epoch [17/20], Step [3600/6979], Loss: 0.5234\n",
            "Epoch [17/20], Step [3700/6979], Loss: 0.3704\n",
            "Epoch [17/20], Step [3800/6979], Loss: 0.4411\n",
            "Epoch [17/20], Step [3900/6979], Loss: 0.3132\n",
            "Epoch [17/20], Step [4000/6979], Loss: 0.7081\n",
            "Epoch [17/20], Step [4100/6979], Loss: 0.3291\n",
            "Epoch [17/20], Step [4200/6979], Loss: 0.5228\n",
            "Epoch [17/20], Step [4300/6979], Loss: 0.4744\n",
            "Epoch [17/20], Step [4400/6979], Loss: 0.4904\n",
            "Epoch [17/20], Step [4500/6979], Loss: 0.5036\n",
            "Epoch [17/20], Step [4600/6979], Loss: 0.3841\n",
            "Epoch [17/20], Step [4700/6979], Loss: 0.4144\n",
            "Epoch [17/20], Step [4800/6979], Loss: 0.4802\n",
            "Epoch [17/20], Step [4900/6979], Loss: 0.3291\n",
            "Epoch [17/20], Step [5000/6979], Loss: 0.4242\n",
            "Epoch [17/20], Step [5100/6979], Loss: 0.6469\n",
            "Epoch [17/20], Step [5200/6979], Loss: 0.5429\n",
            "Epoch [17/20], Step [5300/6979], Loss: 0.2564\n",
            "Epoch [17/20], Step [5400/6979], Loss: 0.5569\n",
            "Epoch [17/20], Step [5500/6979], Loss: 0.5456\n",
            "Epoch [17/20], Step [5600/6979], Loss: 0.3756\n",
            "Epoch [17/20], Step [5700/6979], Loss: 0.4045\n",
            "Epoch [17/20], Step [5800/6979], Loss: 0.4574\n",
            "Epoch [17/20], Step [5900/6979], Loss: 0.5367\n",
            "Epoch [17/20], Step [6000/6979], Loss: 0.3134\n",
            "Epoch [17/20], Step [6100/6979], Loss: 0.6253\n",
            "Epoch [17/20], Step [6200/6979], Loss: 0.4489\n",
            "Epoch [17/20], Step [6300/6979], Loss: 0.3487\n",
            "Epoch [17/20], Step [6400/6979], Loss: 0.4595\n",
            "Epoch [17/20], Step [6500/6979], Loss: 0.5920\n",
            "Epoch [17/20], Step [6600/6979], Loss: 0.6032\n",
            "Epoch [17/20], Step [6700/6979], Loss: 0.4452\n",
            "Epoch [17/20], Step [6800/6979], Loss: 0.5472\n",
            "Epoch [17/20], Step [6900/6979], Loss: 0.6059\n",
            "Epoch [18/20], Step [100/6979], Loss: 0.4332\n",
            "Epoch [18/20], Step [200/6979], Loss: 0.4436\n",
            "Epoch [18/20], Step [300/6979], Loss: 0.5441\n",
            "Epoch [18/20], Step [400/6979], Loss: 0.3517\n",
            "Epoch [18/20], Step [500/6979], Loss: 0.3786\n",
            "Epoch [18/20], Step [600/6979], Loss: 0.4441\n",
            "Epoch [18/20], Step [700/6979], Loss: 0.3220\n",
            "Epoch [18/20], Step [800/6979], Loss: 0.4875\n",
            "Epoch [18/20], Step [900/6979], Loss: 0.5293\n",
            "Epoch [18/20], Step [1000/6979], Loss: 0.7206\n",
            "Epoch [18/20], Step [1100/6979], Loss: 0.3747\n",
            "Epoch [18/20], Step [1200/6979], Loss: 0.3955\n",
            "Epoch [18/20], Step [1300/6979], Loss: 0.4736\n",
            "Epoch [18/20], Step [1400/6979], Loss: 0.4145\n",
            "Epoch [18/20], Step [1500/6979], Loss: 0.5754\n",
            "Epoch [18/20], Step [1600/6979], Loss: 0.3689\n",
            "Epoch [18/20], Step [1700/6979], Loss: 0.5364\n",
            "Epoch [18/20], Step [1800/6979], Loss: 0.5250\n",
            "Epoch [18/20], Step [1900/6979], Loss: 0.3658\n",
            "Epoch [18/20], Step [2000/6979], Loss: 0.4168\n",
            "Epoch [18/20], Step [2100/6979], Loss: 0.4209\n",
            "Epoch [18/20], Step [2200/6979], Loss: 0.3842\n",
            "Epoch [18/20], Step [2300/6979], Loss: 0.4204\n",
            "Epoch [18/20], Step [2400/6979], Loss: 0.4762\n",
            "Epoch [18/20], Step [2500/6979], Loss: 0.4442\n",
            "Epoch [18/20], Step [2600/6979], Loss: 0.7498\n",
            "Epoch [18/20], Step [2700/6979], Loss: 0.4780\n",
            "Epoch [18/20], Step [2800/6979], Loss: 0.3929\n",
            "Epoch [18/20], Step [2900/6979], Loss: 0.3974\n",
            "Epoch [18/20], Step [3000/6979], Loss: 0.3818\n",
            "Epoch [18/20], Step [3100/6979], Loss: 0.4759\n",
            "Epoch [18/20], Step [3200/6979], Loss: 0.3840\n",
            "Epoch [18/20], Step [3300/6979], Loss: 0.5985\n",
            "Epoch [18/20], Step [3400/6979], Loss: 0.3050\n",
            "Epoch [18/20], Step [3500/6979], Loss: 0.5236\n",
            "Epoch [18/20], Step [3600/6979], Loss: 0.3953\n",
            "Epoch [18/20], Step [3700/6979], Loss: 0.2361\n",
            "Epoch [18/20], Step [3800/6979], Loss: 0.4137\n",
            "Epoch [18/20], Step [3900/6979], Loss: 0.3168\n",
            "Epoch [18/20], Step [4000/6979], Loss: 0.4273\n",
            "Epoch [18/20], Step [4100/6979], Loss: 0.4218\n",
            "Epoch [18/20], Step [4200/6979], Loss: 0.5174\n",
            "Epoch [18/20], Step [4300/6979], Loss: 0.3328\n",
            "Epoch [18/20], Step [4400/6979], Loss: 0.4514\n",
            "Epoch [18/20], Step [4500/6979], Loss: 0.5209\n",
            "Epoch [18/20], Step [4600/6979], Loss: 0.4977\n",
            "Epoch [18/20], Step [4700/6979], Loss: 0.4736\n",
            "Epoch [18/20], Step [4800/6979], Loss: 0.4016\n",
            "Epoch [18/20], Step [4900/6979], Loss: 0.2911\n",
            "Epoch [18/20], Step [5000/6979], Loss: 0.3646\n",
            "Epoch [18/20], Step [5100/6979], Loss: 0.4616\n",
            "Epoch [18/20], Step [5200/6979], Loss: 0.3714\n",
            "Epoch [18/20], Step [5300/6979], Loss: 0.4471\n",
            "Epoch [18/20], Step [5400/6979], Loss: 0.4458\n",
            "Epoch [18/20], Step [5500/6979], Loss: 0.4707\n",
            "Epoch [18/20], Step [5600/6979], Loss: 0.3266\n",
            "Epoch [18/20], Step [5700/6979], Loss: 0.4787\n",
            "Epoch [18/20], Step [5800/6979], Loss: 0.2808\n",
            "Epoch [18/20], Step [5900/6979], Loss: 0.2985\n",
            "Epoch [18/20], Step [6000/6979], Loss: 0.3534\n",
            "Epoch [18/20], Step [6100/6979], Loss: 0.6055\n",
            "Epoch [18/20], Step [6200/6979], Loss: 0.3291\n",
            "Epoch [18/20], Step [6300/6979], Loss: 0.4093\n",
            "Epoch [18/20], Step [6400/6979], Loss: 0.5162\n",
            "Epoch [18/20], Step [6500/6979], Loss: 0.5554\n",
            "Epoch [18/20], Step [6600/6979], Loss: 0.2776\n",
            "Epoch [18/20], Step [6700/6979], Loss: 0.5488\n",
            "Epoch [18/20], Step [6800/6979], Loss: 0.5309\n",
            "Epoch [18/20], Step [6900/6979], Loss: 0.3993\n",
            "Epoch [19/20], Step [100/6979], Loss: 0.5169\n",
            "Epoch [19/20], Step [200/6979], Loss: 0.2787\n",
            "Epoch [19/20], Step [300/6979], Loss: 0.5385\n",
            "Epoch [19/20], Step [400/6979], Loss: 0.5314\n",
            "Epoch [19/20], Step [500/6979], Loss: 0.8200\n",
            "Epoch [19/20], Step [600/6979], Loss: 0.4716\n",
            "Epoch [19/20], Step [700/6979], Loss: 0.3252\n",
            "Epoch [19/20], Step [800/6979], Loss: 0.2990\n",
            "Epoch [19/20], Step [900/6979], Loss: 0.6141\n",
            "Epoch [19/20], Step [1000/6979], Loss: 0.4407\n",
            "Epoch [19/20], Step [1100/6979], Loss: 0.4518\n",
            "Epoch [19/20], Step [1200/6979], Loss: 0.6961\n",
            "Epoch [19/20], Step [1300/6979], Loss: 0.5579\n",
            "Epoch [19/20], Step [1400/6979], Loss: 0.3666\n",
            "Epoch [19/20], Step [1500/6979], Loss: 0.3437\n",
            "Epoch [19/20], Step [1600/6979], Loss: 0.4623\n",
            "Epoch [19/20], Step [1700/6979], Loss: 0.3727\n",
            "Epoch [19/20], Step [1800/6979], Loss: 0.5290\n",
            "Epoch [19/20], Step [1900/6979], Loss: 0.4069\n",
            "Epoch [19/20], Step [2000/6979], Loss: 0.3396\n",
            "Epoch [19/20], Step [2100/6979], Loss: 0.3448\n",
            "Epoch [19/20], Step [2200/6979], Loss: 0.3590\n",
            "Epoch [19/20], Step [2300/6979], Loss: 0.4627\n",
            "Epoch [19/20], Step [2400/6979], Loss: 0.3114\n",
            "Epoch [19/20], Step [2500/6979], Loss: 0.4594\n",
            "Epoch [19/20], Step [2600/6979], Loss: 0.4070\n",
            "Epoch [19/20], Step [2700/6979], Loss: 0.3071\n",
            "Epoch [19/20], Step [2800/6979], Loss: 0.5335\n",
            "Epoch [19/20], Step [2900/6979], Loss: 0.6123\n",
            "Epoch [19/20], Step [3000/6979], Loss: 0.3490\n",
            "Epoch [19/20], Step [3100/6979], Loss: 0.3042\n",
            "Epoch [19/20], Step [3200/6979], Loss: 0.4036\n",
            "Epoch [19/20], Step [3300/6979], Loss: 0.3922\n",
            "Epoch [19/20], Step [3400/6979], Loss: 0.3843\n",
            "Epoch [19/20], Step [3500/6979], Loss: 0.3151\n",
            "Epoch [19/20], Step [3600/6979], Loss: 0.3357\n",
            "Epoch [19/20], Step [3700/6979], Loss: 0.2768\n",
            "Epoch [19/20], Step [3800/6979], Loss: 0.4961\n",
            "Epoch [19/20], Step [3900/6979], Loss: 0.3932\n",
            "Epoch [19/20], Step [4000/6979], Loss: 0.3738\n",
            "Epoch [19/20], Step [4100/6979], Loss: 0.3717\n",
            "Epoch [19/20], Step [4200/6979], Loss: 0.4263\n",
            "Epoch [19/20], Step [4300/6979], Loss: 0.5131\n",
            "Epoch [19/20], Step [4400/6979], Loss: 0.5952\n",
            "Epoch [19/20], Step [4500/6979], Loss: 0.4411\n",
            "Epoch [19/20], Step [4600/6979], Loss: 0.3822\n",
            "Epoch [19/20], Step [4700/6979], Loss: 0.3720\n",
            "Epoch [19/20], Step [4800/6979], Loss: 0.3444\n",
            "Epoch [19/20], Step [4900/6979], Loss: 0.5287\n",
            "Epoch [19/20], Step [5000/6979], Loss: 0.5134\n",
            "Epoch [19/20], Step [5100/6979], Loss: 0.4511\n",
            "Epoch [19/20], Step [5200/6979], Loss: 0.4457\n",
            "Epoch [19/20], Step [5300/6979], Loss: 0.3073\n",
            "Epoch [19/20], Step [5400/6979], Loss: 0.6454\n",
            "Epoch [19/20], Step [5500/6979], Loss: 0.3635\n",
            "Epoch [19/20], Step [5600/6979], Loss: 0.4474\n",
            "Epoch [19/20], Step [5700/6979], Loss: 0.3239\n",
            "Epoch [19/20], Step [5800/6979], Loss: 0.2622\n",
            "Epoch [19/20], Step [5900/6979], Loss: 0.2822\n",
            "Epoch [19/20], Step [6000/6979], Loss: 0.3887\n",
            "Epoch [19/20], Step [6100/6979], Loss: 0.4902\n",
            "Epoch [19/20], Step [6200/6979], Loss: 0.5102\n",
            "Epoch [19/20], Step [6300/6979], Loss: 0.6617\n",
            "Epoch [19/20], Step [6400/6979], Loss: 0.6379\n",
            "Epoch [19/20], Step [6500/6979], Loss: 0.5253\n",
            "Epoch [19/20], Step [6600/6979], Loss: 0.4563\n",
            "Epoch [19/20], Step [6700/6979], Loss: 0.3952\n",
            "Epoch [19/20], Step [6800/6979], Loss: 0.4900\n",
            "Epoch [19/20], Step [6900/6979], Loss: 0.4298\n",
            "Epoch [20/20], Step [100/6979], Loss: 0.4270\n",
            "Epoch [20/20], Step [200/6979], Loss: 0.3483\n",
            "Epoch [20/20], Step [300/6979], Loss: 0.3556\n",
            "Epoch [20/20], Step [400/6979], Loss: 0.3935\n",
            "Epoch [20/20], Step [500/6979], Loss: 0.6743\n",
            "Epoch [20/20], Step [600/6979], Loss: 0.4022\n",
            "Epoch [20/20], Step [700/6979], Loss: 0.5011\n",
            "Epoch [20/20], Step [800/6979], Loss: 0.4441\n",
            "Epoch [20/20], Step [900/6979], Loss: 0.4698\n",
            "Epoch [20/20], Step [1000/6979], Loss: 0.5054\n",
            "Epoch [20/20], Step [1100/6979], Loss: 0.3532\n",
            "Epoch [20/20], Step [1200/6979], Loss: 0.4002\n",
            "Epoch [20/20], Step [1300/6979], Loss: 0.3174\n",
            "Epoch [20/20], Step [1400/6979], Loss: 0.4838\n",
            "Epoch [20/20], Step [1500/6979], Loss: 0.2782\n",
            "Epoch [20/20], Step [1600/6979], Loss: 0.4802\n",
            "Epoch [20/20], Step [1700/6979], Loss: 0.4517\n",
            "Epoch [20/20], Step [1800/6979], Loss: 0.4515\n",
            "Epoch [20/20], Step [1900/6979], Loss: 0.4793\n",
            "Epoch [20/20], Step [2000/6979], Loss: 0.4390\n",
            "Epoch [20/20], Step [2100/6979], Loss: 0.5800\n",
            "Epoch [20/20], Step [2200/6979], Loss: 0.3759\n",
            "Epoch [20/20], Step [2300/6979], Loss: 0.2791\n",
            "Epoch [20/20], Step [2400/6979], Loss: 0.4432\n",
            "Epoch [20/20], Step [2500/6979], Loss: 0.3424\n",
            "Epoch [20/20], Step [2600/6979], Loss: 0.5077\n",
            "Epoch [20/20], Step [2700/6979], Loss: 0.4960\n",
            "Epoch [20/20], Step [2800/6979], Loss: 0.4510\n",
            "Epoch [20/20], Step [2900/6979], Loss: 0.4404\n",
            "Epoch [20/20], Step [3000/6979], Loss: 0.4677\n",
            "Epoch [20/20], Step [3100/6979], Loss: 0.3294\n",
            "Epoch [20/20], Step [3200/6979], Loss: 0.5501\n",
            "Epoch [20/20], Step [3300/6979], Loss: 0.5039\n",
            "Epoch [20/20], Step [3400/6979], Loss: 0.3058\n",
            "Epoch [20/20], Step [3500/6979], Loss: 0.4233\n",
            "Epoch [20/20], Step [3600/6979], Loss: 0.4119\n",
            "Epoch [20/20], Step [3700/6979], Loss: 0.4826\n",
            "Epoch [20/20], Step [3800/6979], Loss: 0.3445\n",
            "Epoch [20/20], Step [3900/6979], Loss: 0.3751\n",
            "Epoch [20/20], Step [4000/6979], Loss: 0.4601\n",
            "Epoch [20/20], Step [4100/6979], Loss: 0.5562\n",
            "Epoch [20/20], Step [4200/6979], Loss: 0.4347\n",
            "Epoch [20/20], Step [4300/6979], Loss: 0.3833\n",
            "Epoch [20/20], Step [4400/6979], Loss: 0.3474\n",
            "Epoch [20/20], Step [4500/6979], Loss: 0.3254\n",
            "Epoch [20/20], Step [4600/6979], Loss: 0.3388\n",
            "Epoch [20/20], Step [4700/6979], Loss: 0.4574\n",
            "Epoch [20/20], Step [4800/6979], Loss: 0.4292\n",
            "Epoch [20/20], Step [4900/6979], Loss: 0.5032\n",
            "Epoch [20/20], Step [5000/6979], Loss: 0.3211\n",
            "Epoch [20/20], Step [5100/6979], Loss: 0.5601\n",
            "Epoch [20/20], Step [5200/6979], Loss: 0.4559\n",
            "Epoch [20/20], Step [5300/6979], Loss: 0.3056\n",
            "Epoch [20/20], Step [5400/6979], Loss: 0.4188\n",
            "Epoch [20/20], Step [5500/6979], Loss: 0.5059\n",
            "Epoch [20/20], Step [5600/6979], Loss: 0.2854\n",
            "Epoch [20/20], Step [5700/6979], Loss: 0.3294\n",
            "Epoch [20/20], Step [5800/6979], Loss: 0.3212\n",
            "Epoch [20/20], Step [5900/6979], Loss: 0.5039\n",
            "Epoch [20/20], Step [6000/6979], Loss: 0.5790\n",
            "Epoch [20/20], Step [6100/6979], Loss: 0.3850\n",
            "Epoch [20/20], Step [6200/6979], Loss: 0.2930\n",
            "Epoch [20/20], Step [6300/6979], Loss: 0.4533\n",
            "Epoch [20/20], Step [6400/6979], Loss: 0.3909\n",
            "Epoch [20/20], Step [6500/6979], Loss: 0.3357\n",
            "Epoch [20/20], Step [6600/6979], Loss: 0.2999\n",
            "Epoch [20/20], Step [6700/6979], Loss: 0.4650\n",
            "Epoch [20/20], Step [6800/6979], Loss: 0.4007\n",
            "Epoch [20/20], Step [6900/6979], Loss: 0.3780\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTPvMW5jHB9X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "a3250213-b59a-4ce8-f5f5-3708c4222ec5"
      },
      "source": [
        "#Evaluating the accuracy of the model\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for images,labels in test_gen:\n",
        "  images = Variable(images.view(-1,28*28)).cuda()\n",
        "  labels = labels.cuda()\n",
        "  \n",
        "  output = net(images)\n",
        "  _, predicted = torch.max(output,1)\n",
        "  correct += (predicted == labels).sum()\n",
        "  total += labels.size(0)\n",
        "\n",
        "print('Accuracy of the model: %.3f %%' %((100*correct)/(total+1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-05897ff5ba8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-aa20fbe67c18>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv6\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [10, 1, 3, 3], but got 2-dimensional input of size [100, 784] instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jctkYgyif71W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}